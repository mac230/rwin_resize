# my notes on the CSHL Computational Genomics course

# tool for variant calling is genome analysis toolkit
[[GATK]]

* Tuesday, December 3 2019
A neat opportunity that arose:

"Hi Mahlon,

I’m Patricia with CSHL Meetings & Courses – I hope my email finds you well. I
wanted to reach out and ask if you’d be interested in being the course’s
featured participant.

Every week, we feature a meeting or course participant on our blog and on
Friday, December 13th I’d like to feature a Computational Genomics course
trainee. The Feature would require answering a few questions (which can be done
via email) and a photo of you. If this is of interest, please let me know by
tomorrow (12/6) so I can follow-up accordingly.

Cheers,

Patricia"

* Wednesday, December 4 2019
#+BEGIN_SRC bash
cd
mkdir ./reader/cshl_computational_genomics
cd ~/reader/cshl_computational_genomics
#+END_SRC

** DONE Lecture: Similarity Searching via BLAST
file+emacs:~/reader/cshl_computational_genomics/practical_similarity_searching_001.pdf
-----
BLAST is used to annotate genomes, i.e., assign (putative) function to unknown
regions.

BLAST can be used to search:

    - [1] DNA databases

    - [2] protein databases


-----
FASTA came from Dayoff format.  Basic idea was that it simplified the
format - puts the entire sequence together.


-----
*biological functional knowledge is based on sequence comparison.*
i.e., the function of many proteins is inferred on the basis of
sequence similarity to proteins whose function is *known*

e.g., IF sequence of protein x is similar to TF,
      THEN protein x is prob. a TF


-----
protein function in most species other than yeast/humans is based on
sequence similarity comparisons.  For yeast/humans, ~50% of protein
function is annotated based on /actual/ experimental data.


*** Effective Similarity Searching

    - [1] always search protein databases!!!!
          + more effective/specific than DNA
          + can use translated DNA sequence for this purpose

    - [2] use E value for homology inference
          + better than % homology

    - [3] all methods miss homologs
          + methods are stringent against false-positives
          + BUT, they miss homologs


"for most proteins, homologs are easily found over long evolutionary distances."
I think this is because if you look back far enough, you eventually get to a
common ancestor, even for things that are highly diverged.


*** Sequence Similarity

    - [1] homologous sequences = share common ancestor
          + BUT, most sequences are non-homologous

    - [2] homologous proteins share structure but not /necssarily/ function

    - [3] scoring matrix sets the time scale you look over
          + not all homology need be distant
          + i.e., look at functionally diverged proteins over short scale

    - [4] all methods miss homologs and find homologs the others miss
          + i.e., no BEST method



-----
for most proteins, easy to find a homolog over long evo. distances

difficult for distant relationships or very short distances

most default search parameters are optimized for distant relationships
and work well -> i.e., BLAST works well w/ default settings



*** Homology
*Homologous sequences share a common ancestor.*

types of homologs include:

    - [1] homologs :: proteins that share a common ancestor

    - [2] orthologs :: same protein, different species
                       + subset of homologs

    - [3] paralogs :: functionally diverged proteins in same species
                      + subset of homologs

e.g., if you find a sequence similarity betw. a protein in human and
yeast, it means the gene/protein was present in a common ancestor that
existed billions of years ago.  The gene/protein may have changed
sequence, but was present in lca.

Think about this in the context of e. coli and humans.  The protein
was around 2.5 bya. in lca.


-----
why are proteins homologous?  two possibilities:

    - [1] they share a common ancestor

    - [2] nature needs protein x
          + therefore, these arise in independent lineages
          + functional constraints - organisms need protein x

the idea of common ancestor is /much/ simpler and, therefore, the more
likely explanation.


-----
we can quantify the odds of seeing deviation as large as in our
homologs using expectation values:

    - [1] RMSD - angstrohm-based structure comparison
          + E value for any 2 sequences = ~10 A
          + for the example homologs = 1.6 A
          + E < 10^-84

    - [2] sequence
          + same idea
          + E values
          + returns % identity


-----
We have sequences >>>> structures.  Therefore, we have to do homology
comparisons based on sequence, not structure.

idea: if a room full of people write out 224 letters, the odds that
any 2 people, much less > 2 people, write the same sequence are very
small.


-----
So, what happens when you have significant structural identity but NOT
sequence homology?

If you can see significant homology, you know you have a homolog, if
you don't, it might still be there, but you can't see it.

To truly know, you need a structure.


-----
absence of homology is inferred by different structures.  E value of
both structure and sequence are greater than expected by chance.


-----
homology definitions:

    - [1] protein/gene/DNA that share common ancestor

    - [2] individual positions/columns in a multiple sequence alignment
          + no statistical signif. here
          + DON'T SAY '2 sequences are 50% homologous'
          + INSTEAD '2 sequences are 50% identical'
          + this is the whole 'homology is binary' deal

    - [3] specific characters (i.e., function/morphology)
          + e.g., 'bird and bat wings are not homologous'

We use (1) above b/c we can do stats on it.


-----
if there is a common ancestor, aren't all sequences homologous?

no, in the sense that diff. protein families originate in different
lineages.


-----
common ancestry is inferred on the basis of stat. signf. excess
similarity.


*lack of signif. sequence CANNOT tell you about absence of homology*
*HAVE to have structure to definitively say no homology*


-----
~40% of proteins in e. coli have homology to human proteins (on the
basis of sequence similarity via BLAST)


-----
note the slide on how well protein sequence for homology
identification works compared to DNA.

x axis = divergence time
y axis = n. queries (of 100) that yield signif. hits

DNA sequences can really only go back in time ~2 million years ago


-----
two types of homology:

    - [1] ortholog
          + differences in sequences are due to speciation events
          + this is why the cluster on slide 17 looks so good

    - [2] paralog
          + things produced by gene duplication events

'since we care about function, we like orthologs'


-----
unrelated protein sequences behave like random sequences in homology
searches -> this is fundamental to why BLAST works.

So, unrelated sequences are identified by using random sequences.
This is akin to permuting to create a null distribution.


-----
*the E value is what we use to infer homology*

other parameters we'll get back:

| len    | s-w | bits | E                 | %_id                  | %_sim | alen |
|--------+-----+------+-------------------+-----------------------+-------+------|
| length |     |      | expectation value | same aa at posn.      |       |      |
|        |     |      |                   | 30% is typical cutoff |       |      |


bill likes 10^-3 as cutoff for protein similarity sequence E value

E value is adjusted p value


-----
how can you have less identity but better E value?

the score (s-w) is the reason; higher score = lower E value

the scoring algorithm works by generating the *best possible
alignment*.  this is why the alignment can start at aa 20 in the
human and aa 1 in the e. coli protein, e.g.  the alignment you see is
the best one that can be obtained for the 2 sequences.



*** The PAM250 Matrix
This is the matrix that generates alignments for proteins.

in this matrix, not all identities are equal:

    - [1] cys = 12
    - [2] ser = 2
    - [3] trp = 17

we give higher priority to rare amino acids.  e.g., above, the
liklihood of aligning 2 trps is low, so give it a big score.

also considers mutability.  e.g., can mutate an ala easily w/ little
effect, but can't use anything other than cys for disulfide bond.
thus, aligned cys gets high score.  something like ala is an
"exchangeable" aa.

the triangles on pg. 12 denote clusters of functionally related aa's.
these are denoted as single dots on the alignment visualization.

so, b/c different aa's get different alignment values, you can have
proteins w/ more identity BUT a worse score.  i.e., you'd have a bunch
of common aa's (ala, etc..) to get high identity, but your score
result is not big.

note also that many aa's get neg. scores (e.g., trp to gly)


-----
*homology is transitive*
w/ this slide, the idea is that the homology is monotonic if you move down a
phylogenetic tree.  so, if you align human ATP synthase, it'll get a lower and
lower E value as you move from bacteria to human, as shown in the slide.
vice-versa for comparing e. coli ATP synthase and moving from e. coli to human,
again, as shown.


-----
an interesting edge case is when you identify a short stretch of
protein that is homologous.  This results from the following:

    - [1] your alignment is based on best score
    - [2] this can be a short stretch of AA's when non-homologous
    - [3] results in low E value from homology search on short stretch

You would say the proteins are non-homologous in the example where you
have low E value for 90 aa's of a 3000 aa protein.


-----
the deal with the chloroplast highlighted slides is as follows:

so, the idea is that you do the alignment w/ human ATP synthase and you get
homology with all the animal verions of ATP synthase, BUT, not w/ the
chloroplast ATP synthase.  This is b/c the chloroplast is actually coming from
an ingested bacteria, so it's way diverged from human.  BUT, if you align it to
bacterial ATP synthase, it's way better, b/c they're not that diverged.

** DONE Workshop: Sequence Similarity Searching
:honeybee_protein_FASTA:
>NP_001171499.1 glutathione S-transferase D1 [Apis mellifera]
MPIDFYQLPGSPPCRAVALTAAALDIEMNFKQVNLMNGEHLKPEFLKINPQHTIPTIDDNGFRLWESRAI
MTYLADQYGKNDTLYPKDLKKRAIVNQRLYFDMCSLYKSFMDYYYPIIFMKAVKDQAKYENIGTALSFLD
KFLEGENYVAGKNMTLADLSIVSTVSTLEALDYDLSKYKNVTRWFAKIKPEIPKYEEYNNAGLKMFKELV
NEKLSKK
:END:

*** DONE #1
1. Use the FASTA search page [pgm] to compare Honey bee glutathione
transferase D1 NP_001171499/ H9KLY5_APIME [seq] (gi|295842263) to
the PIR1 Annotated protein sequence database. Use 'search database'
here.


-----
How long is the query sequence?
# 217 aa


-----
How many sequences are in the PIR1 database?
# 13144 seqs


-----
What scoring matrix was used?
# BL50 matrix


-----
What were the gap penalties? (what is the penalty for a
one-residue gap? two residues?)

# open/ext = -10/-2 -> -10 to open a gap and -2 to insert an aa into it recall
# that this is about generating the highest possile alignment score, so you would
# open a gap when doing so nets a higher score (a good example of the whole thing
# about the algorithm knowing nothing about biology)


-----
What are each of the numbers after the description of the library
sequence?

# opt = raw optimized alignment score (higher = more alignment)
# bit = normalized alignment
# %_id = exact identity at a sequence position
# %_sim = >1 alignment score for aa at a position


-----
Which one is best for inferring homology?
# E value


-----
How similar is the highest scoring sequence?
# 0.811


-----
What is the difference between %_id and %_sim? Why is there no
100% identity match?
# the protein you used as query isn't in the database


-----
Looking at an alignment, where are the boundaries of the
alignment (the best local region)?
# starts @ 3 in query seq. @ 2 in aligned seq., denoted by
# "(3-208:2-206)"


-----
How many gaps are in the best alignment? The second best?
# 1, denoted by "-" in the alignment image 18



*** TODO #2
Homologs, non-homologs, and the statistical control.

What is the highest scoring non-homolog? (The non-homolog with the
highest alignment score, or the lowest E()-value.) If the statistical
estimates are accurate, what should the E()-value for the highest
non-homolog (the highest score by chance) be? (This is a control for
statistical accuracy.)

we expect this value to be ~1.



You can use the domain diagrams (colors) to identify distant homologs, and, by elimination, the highest scoring non-homolog. You can also use the Sequence Lookup link to Uniprot to look at Domains and Families. (Note that domain colors are recycled -- light green is not only C.Thioredoxin, but also the unrelated C.GCS and C.Glyco_hydro_tim.)

    What is the E()-value of the most distant homolog shown (based on displayed domain content)? Could there be more distant homologs?
    How would you confirm that your candidate non-homolog was truly unrelated? (Hint - compare your candidate non-homolog with SwissProt or QFO78/Uniprot Ref for a more comprehensive test.)

** DONE Lecture: Sequencing Technologies
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_DNA_Sequencing.pdf

*illumina is great for quantification b/c of count accuracy*

amplification creates issues:

    - [1] polymerase error

    - [2] not sequencing native DNA
          + so lose things like modifications

ALL sequencing methods are limited by being able to read only a
portion of a given molecule.  i.e., you don't sequence an entire
chromosome.  Best technologies read up to 1 mb.


-----
illumina is still cheapest per bp, but limitation is that they're
short reads.


*** Sanger and Pyro Sequencing
thinking about probability of terminating at a specific nt.

if we have 5% terminating nt, then:

p(term. @ 1st nt) = 0.05
p(term. @ 2nd nt) = 0.95 * 0.05 (b/c you didn't term on nt #1)
p(term. @ 3rd nt) = 0.95 * 0.05 * 0.05
...
p(term. @ nth nt) = 0.95^(n-1) * 0.05

so, for the 100th nt

0.95^99 * 0.05 = 3.116e-4

so, p that you terminate before 100th = sum of all probabilities prior
to 100th (i.e., essentially 1 for long molecules)

the above is true b/c it's the p(any event other than the one you
want), which requires adding up all the probabilities.

BUT, w/ millions of molecules, you may still see the molecule you want

#+NAME: sanger_probability_calc_and_plot
#+BEGIN_SRC R

bases <- 1:100
form  <- function(x){0.05 * 0.95^(x - 1)}

## p you terminate before this base
out   <- 1 - sapply(bases, form)
plot(bases, out)


## so, a 1:3200 chance you see terminate at this base
1/(1 - out[100])


## -----
## now go even longer
bases <- 1:1000
form  <- function(x){0.05 * 0.95^(x - 1)}

## p you terminate before this base
out   <- 1 - sapply(bases, form)
1/(1 - out[1000]) ## inf
1/(1 - out[500])  ## 2.6e12
#+END_SRC

-----
*"all sequencing is probabilistic"*


-----
base quality score is only about confidence that the base you read is
the base in the molecule -> tells you nothing about what chromosome,
what region of the genome, etc... it came from.


-----
'sequencing by synthesis'

    - [1] denature the DNA and extend w/ polymerase
    - [2] read out the nucleotide that's incorporated to get sequence

pyrosequening does this in real-time, rather than doing it in post-,
like in Sanger, where you run a gel.


-----
high peaks in pyrosequencing correspond to 'runs' of the same
nucleotide.  in principal, you'd expect a 3x peak height for 3 T's,
e.g., but not true in practice.

pyrosequencing bad at 'homopolymer runs' for this reasons.  becomes a
real problem at the limits of the detector (i.e., can't tell 9 A's
from 10 A's).


-----
benefit of pyrosequencing was big per base cost reduction.



*** Illumina
----------
/Illumina/
----------
molecules are ligated randomly to a slide and the idea is that you add
reagents in such proportions that you can resolve individual molecules
based on where they land on the slide.

    - [1] fold-back PCR generates "clusters" of the same sequence
          + need amplification to get signal

    - [2] sequencing by synthesis
          + reversible terminators

    - [3] add all labeled nt's at the same time


-----
examples for type of sequencing you choose:

    - [1] CHIP-SEQ
          + short read (illumina) b/c DNA substrate is short

    - [2] gene expression
          + short read b/c counts are high (therefore accurate)

    - [3] genome assembly
          + long + short b/c need accuracy and contiguity


-----
2 vs. 4 channel chemistry

when you're imaging, you don't do 4 simultaneously, instead, you
change the filter as you go.

two channel lets you have the same readout w/ less fluors

|---------+-------+---+-------+-----|
|         | A     | G | T     | C   |
|---------+-------+---+-------+-----|
| Image_1 | green |   | green |     |
| Image_2 | red   |  |       | red |
|---------+-------+---+-------+-----|

notice how there is a unique readout associated w/ ea. base.  /and/
this gives you a big gain in speed.


-------------
/Ion Torrent/
-------------
ion torrent attempts to overcome the limitations by moving away from a
light-based readout of sequence.  very similar to pyrosequencing.

tough to scale to illumina levels and also has homopolymer problems



---------------
/Illumina Iseq/
---------------
1 channel chemistry

    - [1] 1st channel image

    - [2] change the chemistry

    - [3] 2nd image

so, you don't need filters

|---------+-------+---+-------+-------|
|         | A     | C | G     | T     |
|---------+-------+---+-------+-------|
| image_1 | green |   | green |       |
| image_2 |       |   | green | green |
|---------+-------+---+-------+-------|

modification in step removes label from A and translocates to C.


-----
throughput for this is reduced b/c you can't cluster as densely.
read-length is similar, though.

might be worth it to test library preps....



*** Pac-Bio Long Read Sequencing
----------------------
/Long-Read Sequencing/
----------------------
pac-bio = single molecule real-time sequencing

error rate is 15%, which is the main limitation of this technology.

methylated bases take longer to incorporate labeled nucleotide, so can
actually read these out via pac-bio technology. (but need to know the
variance for these time shifts to say this w/ confidence).


-----
genome assembly approach:

    - [1] pac-bio long read
          + get genome regions/chromosomes at large scale

    - [2] illumina short read
          + get accurate sequence information


-----
pac-bio smrt reads

create a circular DNA molecule by ligating adapters.  Then, repeatedly
sequence the circle to get consensus sequence information for the
DNA.  'burns' some of your reads to get truly accurate sequence
information.  as w/ all sequencing tech., tradeoff
betw. coverage and accuracy.  w/ 10 passes, get good phred score as
shown in the box plot in the slides.


*** Oxford Nanopore
minion sequencing

uses a motor protein to feed ss-DNA through the protein nanopore to
get the sequencing read.

512 individual channels in a minion setup (includes redundant pores -
4 redundant pores/channel).


-----
ligate the motor protein to your DNA.  motor protein finds the pore
and feeds the DNA through the pore for sequencing.

the ultimate readout is ion flow through the pore.  ea. nt has a
unique signature of change in current flow.


-----
an advantage of this technology is that you get multiple measures of
the same base.  the pore reads multiple bases at the same time.  thus,
you get reads of the same base multiple times, increasing confidence
in your sequence:

ACGTATGCATA
--- read 1
 --- read 2
  --- read 3
   --- read 4

above, reading 3 bases at a time, so, e.g., get 3 reads of base #3
('G').


-----
so, going from current change to bases requires that ea. 3-mer has a
unique current signature.

a key piece here is that the sequence constrains what 3-mers you can
have in a sequence.  so, if you have 'AGT', the next can only be 1 of
the following:

1. GTA
2. GTC
3. GTG
4. GTT

find the one that has the highest probability given the observed
data.

1st example is a hidden markov model, but now apparently use neural
networks.


-----
oxford nanopore also has problems w/ homopolymer runs, as shown by the
big black line blocks (corresponding to non-called bases) in the slide
(segey koren slide).


-----
can do RNA seq via nanopore.

often includes a reverse transcriptase step -> this helps deal w/ RNAs
that have a high degree of 2ndary structure.

uses the same flow cell as DNA sequencing via nanopore



*** High-Throughput Sequencing Data Formats
data generation steps:

    - [1] primary
          + base calling, quality control, filtering, trimming

    - [2] secondary
          + map reads to reference
          + or, de novo assembly

    - [3] tertiary
          + discover variants
          + discover enriched regions
          + discover enriched peaks
          + clustering
          + etc..


-----
standard format of sequencing data is fastq

the plot w/ SSSSS, LLLLL, etc... shows the range of quality scores for
diff. sequencing platforms.

want at least 20 for quality data

"do you have enough good data to still do your experiment after you
throw the crap out?"

** DONE Workshop: Genome Data 1
#+BEGIN_SRC R

## -----
## setup
setwd("~/")


## -----
### Dependencies
## install.packages("cowsay")
## library(cowsay)
## say("Hello world!")


## -----
## Reading in the data
url = "http://fasta.bioch.virginia.edu/cshl/data/hum_refseq_genes.tab"
download.file(url = url, destfile = "./data.txt")
df = read.table("./data.txt", header = T)


## -----
## Data Exploration
dim(df) # prints the dimenstion of the data frame
head(df) # prints the first 6 lines of the data frame

df$acc
unique(df$acc)
length(df$acc)
length(unique(df$acc))

mean(df$g_start)
summary(df)

df$length = (df$g_end - df$g_start)
head(df)

plot(df$ix, df$length)

unique_col = apply(X = df, MARGIN = 2, FUN = unique)
unique_col$chrom
df_1 = subset(df, df$chrom == "chr1")

hist(df_1$length, breaks = 100, xlim = c(1,10000))
hist(log(df_1$length))
mean(df_1$length)

split(df, df$acc)

m = do.call(rbind, lapply(split(df,df$acc), function(x) {return(x[which.max(x$length),])}))
hist(log(m$length), breaks = 100, xlim = c(1,10000))
hist(log(m$length))
mean(m$length)

install.packages("tidyverse")
library("tidyverse")
df %>%
  group_by(acc) %>%
  filter(ix == max(ix))

lapply(split(df, df$chrom), function(x) length(unique(x$acc)))

#+END_SRC

* Thursday, December 5 2019

** DONE Lecture Genome Assembly I: Long Reads
file+emacs:~/reader/cshl_computational_genomics/CSHL_2019_Assembly_001.pdf
How do we determine the sequence of chromosomes that are 100's to 1000's of kb
long?

"jones and pevzner: an introduction to bioinformatics algorithms" (2004)


--------------
/Graph Theory/
--------------
G = {V,E} says a graph (G) is a set of vertices (V [or nodes]) and edges (E).
V = {1..12} says the vertices are the set of integers 1..12
E = {(1,2), (1,9)} says the edges are the connections betw. (1,2), (1,9),
etc...

graphs can be connected entirely or have multiple sets of unconnected graphs
w/in the larger graph.

"if you can reduce your problem to a graph, you can solve it using
well-developed graphy theory algorithms."

"much of computer science is taking one problem and turning it into something
else that can be solved using existing tools."


-----
NP-complete -> n polynomial time
this is very slow.  if, e.g., the algorithm scales proportionately to the number
of nodes, it is linear.  could be square, cube, etc..., but polynomial is
exceptionally slow.

    - [1] Eulerian path :: a path that visits every edge in a graph exactly once
                           + requires exactly 0 or 2 nodes w/ odd degree

Key advantage to Eulerian path is that the algorithm to find it is linearly
proportional to the number of edges.



*** Sequencing
limitations of sequencing:

    - [1] need to amplify to get many copies

    - [2] limited read length
          + e.g., short read

methods to sequence longer molecules:

    - [1] break DNA into fragments
          + this is dependent on breakage happening ranomly (i.e., not biased)

    - [2] then sequence both ends of the fragments

    - [3] assemble the entire sequence based on the fragments


-----
assmebly as a string problem:

all reads came from the same string, so we need the /superstring/ of all of our
reads -> all our reads came from the superstring, so just need to put the
superstring back together.

there are infinitely many superstrings -> recall that we are doing assembly, so
don't have a reference genome to align to -> thus, we could make a superstring
that simply contains our last read appended 1,2,3...inf times.

we want the shortest possible superstring where the overlap amongst all reads is
maximized.

however, the genome is not parsimonious and a solution that collapses to the
simplest superstring will miss, e.g., repeats.


-----
graph representation of the superstring:

    - [1] vertices = the n strings (or reads)

    - [2] edges = weight - overlap(s_i, s_j)
          + overlap = length of longest *prefix* of s_j,
	    a *suffix* of s_i
          + so, pair w/ large overlap = small weight

    - [3] find the shortest path that visits every vertex once

shortest superstring suffers from the limitations above re: repeats, errors in
sequencing reads, and it's not efficient.

we get around this by adding heuristics, which are not optimal, but mostly
work.


-----
layout the graph based on reads:

    - [1] nodes are reads, edges are similarity scores

    - [2] layout: find a path through the graph that explains every read while
      maxmizing quality of overlap (Hamiltonian path problem)

this is followed by 'consensus'ing', where we examine multiple reads to
determine the consensus sequence for a region (i.e., deal w/ sequencing errors).


------------
/Continuity/
------------
If not all of the sequence is represented in reads, we can't resolve the whole
sequence.  This means that the ultimate result is an unconnected graph.  This
leads to a set of contigs.

Other methods are needed to define the orientation of the contigs.

    - [1] contig :: a continuous set of nt's we are confident came from the same molecule
                    + may be unconnected from other contigs


-----
the plot example (grey/red histo) shows the sampling problem of genome
assembly.  The idea is that if we drop balls randomly into 'slots' in the
distribution (1..1000), we don't get equal coverage/peak height.  the histogram
shows the number of reads per region (so, 350 regions we don't cover at n
= 1000).


-----
for the shotgun assembly slides w/ horizontal black lines, the reads images show
areas where you have reads (black) and areas you don't get sequence from
(gray).





** DONE Lecture: Practical Sequence Similarity Searching II
file+emacs:~/reader/cshl_computational_genomics/practical_similarity_searching_002.pdf

    - goal :: how do we do similarity searching effectively
	      + algorithms
	      + scoring matrices

*you are a homologous protein if you have a homologous domain*
*orthologs and paralogs are types of homologs*
*for a matrix, shallow means going back less far in evo. time*
*I think a deeper matrix leads to lower scores overall, but dist. may be a*
*different story*


-----
*use BLOSUM62 and BLAST if you don't know anything about a protein*


-----
alignment is an iterative process where you start at the beginning of each
sequence and align.  you eventually get a better 'score' by introducing gaps,
shifts, etc...

alignments:

    - [1] global :: align the whole sequence

    - [2] local :: look at sub-strings of the sequence


-----
steps in algorithms for alignment:

    - [1] continue diagonal

    - [2] gap

    - [3] start over (local only)


---------------------
/Effective Searching/
---------------------
Questions to ask:

    - [1] what question to ask: is there a homologous protein?

    - [2] does homolog have similar function?

    - [3] does XXX genome have YYY (kinase, gpcr, etc...)


Questions you can't ask:

    - [1] does this DNA sequence have regulatory element x?
          + too short -> never significant

    - [2] does (non-signf.) protein have a similar function/modification
          + this is not homology



-------------
/What to Run/
-------------
pearson strongly encourages not running DNA BLAST.

instead, you can translate DNA sequences to protein sequences.

*BLASTX and protein BLAST*

if you have a coding DNA sequence, translate it and BLAST the translated
sequence.


-----
never search the non-redundant protein sequence database (the default option).

for ebi, don't use the uniprot knowledgebase

*you should search the smallest database that will have homologs of your
protein*.

swissprot is richly annotated, so prob. get lots of functional information on
your homologs.

general goal seems to be find a database that is as small as possible, while
also ID'ing homologs for your protein of interest.



*** Why Smaller Databases
The plot shows the distribution of alignment scores.

The plot shows real scores w/ real proteins; the line shows the idealized
distribution function (so it fits pretty much ideally).

Note the 3 y axes, but the same fit to the distribution looks like.

components of the formula:

    - [1] λ = scaling factor for scoring matrix
          + needed to normalize scores

    - [2] m,n = length of each aligned sequence
          + the length of the sequences affect how likely we are to get
            alignments purely by chance

    - [3] P is generated from 'extreme value distribution'
          + P of getting this alignment value in 1 alignment
            - *BUT* we have to correct for N alignments, where N is the size of
              our database (and hence the number of alignments we do)

    - [4] expectation value = /p/ * database size (Bonferroni correction)

*RECALL AGAIN THAT WE USE 0.001 AS THE CUT VALUE FOR HOMOLOGY*

*SO, IF YOU USE THE GIANT DATABASE, YOU OFTEN DON'T FIND HOMOLOGY, BUT RECALL
THAT YOU DON'T GET TO SAY NON-HOMOLOGOUS, ONLY THAT WE COULDN'T FIND HOMOLOGY*

*YOU WANT 50 BITS* -> good criteria for statistical significance


------------------
/Scoring Matrices/
------------------
for all matrices, the bigger the number, the more change you are looking for and
vice-versa.

the opposite is true for BLOSUM (big = small change, little = big change)

1 PAM = 1 percent change

so for the DNA transition probability slide:

    - [1] 0.99 probability no change (0.99)
    - [2] 0.001 A>C OR 0.001>A>T
    - [3] 0.008 A>G
    - [4] 0.001 + 0.001 + 0.008 + 0.99 = 1.00

you can change the matrices for these by raising the matrix to the PAM score you
want.  See the slide w/ the various PAM matrices.

the different matrices allow you to make predictions based on rate of change.

we then use the numbers in these matrices to calculate an alignment score.

*scoring matrices are a model for how much change there has been*
*the matrix is, e.g., PAM1, PAM10, PAM25, etc...*

PAM250 = 250% change
PAM40 = 40% change = 60% identical

so, a lot more change in the PAM250 and this leads to smaller numbers on the
diagonal.


-----
how can something be 250% different:

imagine 100 balloons on a wall and 250 darts.
1. you hit a dart w/ every throw and throw all darts
2. if you hit the same balloon twice, it returns to the original sequence


-----
so, the diagonals on the PAM250 have much lower values than the diagonals on the
PAM40 model.  if you need some minimum alignment value, you're going to have a
hard time getting it if you assume lot of change (as in PAM250).  if you switch
to shallower matrix (less change [PAM40]), it's easier to get the score you
need.

the "empirical matrix performance" slide shows this.


-----
the bovine example shows how using a different scoring matrix /really/ changes
the alignment b/c you're always trying to get the best score and this depends
critically on the matrix scoring system.


------------------------------------
/Overextension Into Random Sequence/
------------------------------------
this slide shows how you get spurious alignment w/ random sequences flanking
real homology when you use an algorithm that looks for stuff to be distantly
related (in this case, BLOSUM62).  Doesn't give bad enough score for mismatches
(b/c you're looking for lots of change).

you detect this by splitting your region into subregions:

    - [1] real homology = high bit score for sub-region
    - [2] overextended = low bit score for sub-region



*percent identity determines what matrix you use?*
60% = BLOSUM60?


if you have a high degree of identity and use a matrix that looks for low degree
of identity, you'll see spurious over-alignment.  the matrix matters.

*all methods miss homologs and find homologs the other methods miss*

*you are a homologous protein if you have a homologous domain*
*orthologs and paralogs are types of homologs*


**** DONE Questions [100%]
    - [X] darts = mutational events -> yes
    - [X] constant p of bit score? -> it's a constant 40 E score



** TODO Workshop: Advanced Alignment and Scoring Matrices
https://fasta.bioch.virginia.edu/mol_evol/index.html
:honeybee_GST:
>NP_001171499.1 glutathione S-transferase D1 [Apis mellifera]
MPIDFYQLPGSPPCRAVALTAAALDIEMNFKQVNLMNGEHLKPEFLKINPQHTIPTIDDNGFRLWESRAI
MTYLADQYGKNDTLYPKDLKKRAIVNQRLYFDMCSLYKSFMDYYYPIIFMKAVKDQAKYENIGTALSFLD
KFLEGENYVAGKNMTLADLSIVSTVSTLEALDYDLSKYKNVTRWFAKIKPEIPKYEEYNNAGLKMFKELV
NEKLSKK
:END:

[[https://fasta.bioch.virginia.edu/fasta_www2/fasta_www.cgi?rm=select&pgm=fa&query=295842263&db=p&annot_seq2=5][FASTA_program]]

*** #1
1. Use the FASTA search page [pgm] to compare Honey bee glutathione transferase D1 NP_001171499/ H9KLY5_APIME [seq] (gi|295842263) to the PIR1 Annotated protein sequence database.

Take a look at the output.

How long is the query sequence?
217 AA

How many sequences are in the PIR1 database?
under "Library" line: 13144 seqs.

What scoring matrix was used?
BL50 matrix

What were the gap penalties? (what is the penalty for a one-residue gap? two
residues?)
-10 to prepare to insert a gap, -2 to insert the gap, so -12 total

What are each of the numbers after the description of the library sequence?
opt = raw, non-normalized alignment score
bits = bit score
E = expectation score
%id = percent identical sequence
%sim = percent similar sequence
alen = length of aligned sequence


Which one is best for inferring homology?
E value

How similar is the highest scoring sequence?
0.811

What is the difference between %_id and %_sim?
actual identity vs. close identity

Why is there no 100% identity match?
the query sequence isn't in the database

Looking at an alignment, where are the boundaries of the alignment (the best
local region)?
(3:208, 2:206)

How many gaps are in the best alignment? The second best?
1


/Homologs, non-homologs, and the statistical control./

What is the highest scoring non-homolog? (The non-homolog with the highest
alignment score, or the lowest E()-value.)
1.7

If the statistical estimates are accurate, what should the E()-value for the
highest non-homolog (the highest score by chance) be? (This is a control for
statistical accuracy.)
1

You can use the domain diagrams (colors) to identify distant homologs, and, by
elimination, the highest scoring non-homolog. You can also use the Sequence
Lookup link to Uniprot to look at Domains and Families. (Note that domain colors
are recycled -- light green is not only C.Thioredoxin, but also the unrelated
C.GCS and C.Glyco_hydro_tim.)

What is the E()-value of the most distant homolog shown (based on displayed
domain content)? Could there be more distant homologs?


How would you confirm that your candidate non-homolog was truly unrelated?
(Hint - compare your candidate non-homolog with SwissProt or QFO78/Uniprot Ref
for a more comprehensive test.)

Domains and alignment regions

There are three parts to the domain display, the domain structure of the query
(top) sequence (if available), the domain structure of the library (bottom)
sequence, and the domain alignment boundaries in the middle (inside the
alignment box). The boundaries and color of the alignment domain coloring match
the Region: sub-alignment scores.

Note that the alignment of Honey bee GSTD1 and SSPA_ECO57 includes portions of
both the N-terminal and C-terminal domains, but neither domain is completely
aligned. Why do you think the alignments do not include the complete domains?
They are too distantly related to align

Is your explanation for the partial domain alignment consistent the the argument
that domains have a characteristic length? How might you test whether a complete
domain is present?

In the subalignment scores, the Q value is -10 * log(p) for the sub-alignment
score, so Q=30.0 means p < 0.001.


Repeat the GSTD1 search [pgm] using the BLASTP62/-11/-1 scoring matrix that
BLAST uses. Re-examine the SSPA_ECO57 alignment. Are both Glutathione
transferase domains present?
no, the blue domain is absent

Look at the alignments to the homologs above and below SSPA_ECO57. Based on
those aligments, do you think the Glutathione-S-Trfase C-like domain is really
missing? Why did the alignment become so much shorter?
We used a

Examine how the expectation value changes with different scoring matrices
(BlastP62, VT160) and different gap penalties. (The default scoring matrix for
the FASTA programs is BLOSUM50, with gap penalties of -10 to open a gap and -2
for each residue in the gap - e.g. -12 for a one residue gap).

What happens to the E()-value for the close Honey bee GSTD1 vs GSTT1_DROME
alignment with the different matrices and different gap penalties? If the score
decreases, does the E()-value always get worse (increase)?

What happens to the E()-value for distant homologs, like GSTA1_RAT with the
different matrices and different gap penalties?

What happens to the E()-value for the highest scoring unrelated sequence with
the different matrices?


*** DONE #2
2. Do the same Honey bee GSTD1 search (295842263) using the Course BLAST [pgm]
   WWW page.

Take a look at the output.
# note that the rank order of the proteins is the same
# i.e., you can use BLAST or FASTA and get the same result
# Bill says use BLAST
# if your scores/rank order are different, check the matrix used


How long is the query sequence?
# 217

How many sequences are in the PIR1 database?
# 13143

What scoring matrix was used?
# BLOSUM62

What were the gap penalties?
# existence = 11, extension = 1

What are the numbers after the description of the library sequence? Which one is
best for inferring homology?
# E value

Looking at an alignment, where are the boundaries of the alignment (the best
local region)?
# the GST domains; best is the blue GST.C

What is the highest scoring non-homolog?
# 0.34

How do the BLASTP E()-values compare with the FASTA (BLASTP62) E()-values for
the distantly related mammalian and plant sequences?
# BLASTP have lower (more significant) E values

*you are a homologous protein if you have a homologous domain*
*orthologs and paralogs are types of homologs*



*** TODO #3
# a general principle here is that homologous domains can 'pull' other regions
# into alignemnt that are not actually homologous.  you need to pull the
# putative spurious aligner out here and re-run the alignment.  this effect is
# dependent on the scoring matrix that you use.  the algorithm 'overextends'
# into non-homologous regions to optimize score

3. Exploring domains and alignment over-extension -- cortactin (SRC8_HUMAN)

Compare SRC8_HUMAN [pgm] (human cortactin) to the SwissProt protein sequence
database.

Looking at the colored rectangles to the right of the list of best scores, what
are the green domains and blue domains?
# green = HS1_repeat; green = HS1 repeat domain; blue = SH3/SRC homology domain
# all proteins w/ blue SH3 domain are homologs

How many proteins have homologous green domains?
# 6 (homologous = only significant)

How many significant alignments only have a blue domain? Do you think those
proteins are homologous?
# many

Looking at the top five alignments, how many cortactin orthologs do you see?
(ortholog, same protein, different species).
# 3

In the SRC8 HUMAN:CHICK alignment, both the query and the subject (library)
sequences align seven cortactin domains and an SH3 domain. In addition, two
regions (one before the cortactin domain cluster and one after) are well
conserved, but do not have annotated domains (NODOM). Are these non-domain
(NODOM) regions as well conserved as the annotated domains?
# yes, comparable raw, bit, and Q scores, but less identity

Look at the SRC8_HUMAN:HCLS1_MOUSE alignment. How many cortactin domains does
HCLS1_MOUSE contain? How much score does the NODOM between the cortactin domains
and the SH3 domain contribute?

Why is it included in the alignment? Is it likely to be homologous?

Is the NODOM between the cortactin domains and the SH3 domain likely to be
homologous in the SRC8_HUMAN:DBNLB_XENLA alignment?

What data would convince you that the sequences were homologous?

What scoring matrix should be used to reduce over-extension from the SH3 domain?



*** #4
4. Significant similarities within sequences - domain duplication in Calmodulin

Use lalign to examine local similarities between calmodulin CALM1_HUMAN and
itself.

Use plalign to plot the same alignment. How many repeats are present in this
sequence.

What happens to the domain alignment plot when you use a shallower scoring
matrix (try BP62, MD20).


*** #5
5. Exploring domains and over-extension with local alignments -- death
   associated protein kinase (DAPK1_HUMAN)

Look up the domain structure of DAPK1_HUMAN at Pfam [pgm].

What are the major (PfamA) domain regions on the protein?

Which of the domains is repeated?

In a local (LALIGN) alignment, where would you expect to see overlapping domains
like those in Calmodulin (CALM1_HUMAN) and Cortactin (DAPK1_HUMAN)?

Use lalign/plalign [pgm] to examine local similarities between DAPK1_HUMAN and itself. Check the options to "annotate sequence 1 domains" and "annotate sequence 2 domainss". Annotate one of the sequences with "Interpro Domains/UniProt features", and the other with "Uniprot Domains/Uniprot Features". Do you see the domains you expected from Pfam? Do they map in the same places?

Repeat the LALIGN/PLALIGN analysis lalign/plalign [pgm], but select the subset
of the protein where the repeated domains are found (350-700) on both the query
(first) and subject (second) sequence. Looking at the first or second
non-identical self-alignment:

What is the overall percent identity of the alignment?

What is the range in identity accross the different aligned Ankryin domains?

Do the ends of the first alignment correspond to the domain boundaries?

How long are the ankyrin domains?

Based on the percent identities you saw in part (c), what would the appropriate
scoring matrix be to accurately identify the ankyrin domains?

Using a "correct" scoring matrix, are the alignment boundaries more accurate?

What is the percent identity of the alignment (did you pick the right matrix?)


*** #6
6. -- Searching for sequences with known structure -- death associated protein
   kinase (DAPK1_HUMAN)

Search [pgm] the protein structure database (PDB Structures - NCBI) using the
DAPK1_HUMAN protein.

How much of the protein has a known structure?

To double check your answer, search [pgm] the PDB structure sequences using the
three domain regions (Kinase, Ankyrin, Death) identified by Pfam the local
domain plots.

Are there homologs to the Death domain with known structures?

Try searching the protein structure database with a glutathione S-transferase
sequence (e.g. GSTT1_DROME [pgm] or GSTM1_HUMAN [pgm]) or a calmodulin sequence
(CALM1_HUMAN [pgm]), annotating both the query and PDB database. How well do the
Interpro domains line up with the structural domains.





** DONE Lecture: Genome Assembly Part II - Short Reads
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_Assembly_002.pdf
# starts on pg. 72 of the document


*** Assembly Quality

    - [1] coverage :: number of times we've sequenced ea. base in genome
                      + how deeply we've sequenced the genome
                      + coverage at any base could be different than avg. depth
                      + how deeply to sequence
                        - minimize probability of a base
                        - follows a Poisson distribution
                      * coverage = L*N/G
                        - L = read length
                        - N = number of reads
                        - G = genome size

the probability of any base being sequenced is modeled with Poisson with x = 0
(not sequenced).  see the table for the relationship of coverage w/ % bases
sequenced.

see the histograms for more information on this as well.

P(x = 0) = probability of a base being covered 0 times


-----

    - [1] N50 contig lenth :: assembly quality measure
                              + size of the contig that covers 50% of genome

so,
    - L = total length of assembly
    - l/2 = 50% of assembly
    - look contig at which we hit 50% coverage
    - length of this contig = N50

N50 = 20 kb means 1/2 your contigs > 20 kb
*the longer contigs are more useful, so a bigger N50 = better assembly*
b/c your contigs cannot be joined, longer means you're better able to tell what
genes you have (i.e., less genes 'broken' by contig break).


-----
The length of the assembly is always a bad measure of assembly quality b/c you
can always make a longer assembly.  Hence, the N50.




*** Assembling with Short Reads
Initial issues:

    - [1] overlap -> many more comparisons to make
          + more computationally expensive

OLC = overlap, layout, consensus issues


-----
for ea. node as a k-mer, you need a sufficiently long k-mer for the approach to
work.


-----
clarifying the definitions of paths:

    - [1] Hamiltonian :: visits every node/vertex once

    - [2] Eulerian :: visits every edge once
                      + scales proportionately


-----------------
/Velvet Assembly/
-----------------

    - [1] get short read sequence data

    - [2] read out all k-mers
          + build table

    - [3] count occurrence of k-mers
          + add to k-mer hash table

    - [4] remove linear stretches from graph
          + assume this is sequencing error

    - [5] can't resolve the bubble in the graph

"we can only resolve lengths that are shorter than our repeat length"
"unless you make the reads longer, you can't increase contig size"
# see the plot w/ the plateaus on max N50 length
"variation in the genome is very limiting to assembly"
"low heterozygosity helps improve assembly"


-------
/Quake/
-------

    - [1] take a k-mer of a certain size
    - [2] plot how frequently you see it vs. coverage
    - [3] call sequencing errors k-mers you see very rarely
    - [4] use true k-mer distribution to correct errors


*** Hybrid Assembly
A combination of:

    - [1] long read to get contigs
    - [2] short read to get accurate single base calls
          + this uses O-L-C approach

In today's world to get a genome assembled:

    - [1] 60x illumina (paired end)
    - [2] 20+ w/ long reads
          + error rate doesn't matter here
    - [3] if possible: 10x linked reads, Hi-C data
          + good for phasing

*heterozyous structural variation (NOT SNPS) are the issue for
haplotyping/assembly*





** DONE Workshop: Dataset Collections and Mapping in Galaxy

Key_points:

    - [1] "Jobs running on data collections use the same settings for each
      dataset in the collection."

    - [2] "For example, FASTQ files can be combined into lists of dataset
      pairs. Each pair is made up of the forward reads dataset and the reverse
      reads dataset."

    - [3] "In this tutorial, several tools were run on the list of dataset
      pairs, such as bwa-mem, cleanSam, Filter SAM or BAM, etc."

    - [4] "When using collections, you have to click on the batch input mode
      button, to select one of the collections available in the
      history. Otherwise, the collections are not available in the drop-down
      list."


-----
In this example, we have libraries of data from a mother and child that have
genomic DNA isolated from blood and cheek (where, F = forward reads, R =
reverse).

    M117-bl_1 - family 117, mother, 1-st (F) read from blood
    M117-bl_2 - family 117, mother, 2-nd (R) read from blood
    M117-ch_1 - family 117, mother, 1-st (F) read from cheek
    M117-ch_1 - family 117, mother, 2-nd (R) read from cheek
    M117C1-bl_1- family 117, child, 1-st (F) read from blood
    M117C1-bl_2- family 117, child, 2-nd (R) read from blood
    M117C1-ch_1- family 117, child, 1-st (F) read from cheek
    M117C1-ch_2- family 117, child, 2-nd (R) read from cheek


-----
So, the idea here is that you can merge multiple datasets (generated in the same
way) into a single dataset that gets processed identically.  Here, we have 8
sequencing lanes that we pair into F/R pairs on a per sample basis.  Then, we
align the pairs to a reference genome.  After this, we clean the assembly, and
downsample it to generate a file we can visualize.  We downsample the data
(taking 5% of it) to actually visualize.

The result here was that the researchers wanted to sequence the mitochondrial
genome.  To do so, they amplified mitochondrial DNA by PCR.  The gaps in the
alignment map are the boundaries of the primers (there are 4 amplicons).

Other observations:

    - [1] the different colors correspond to DNA strand being read
          + think about how you can still get this w/ paired end sequencing

    - [2] you could, in principle, map the different samples using the tags, but
      this was not the point of the exercise, so we didn't do it.


** DONE Lecture: Statistics 1: Experiments, Data, Visualization
file+emacs:~/reader/cshl_computational_genomics/CSHL_2019_statistics_001.pdf
https://www.jhudatascience.org
#+NAME: statistics_goals

    - [1] exploratory analysis
          + plotting
          + skepticism

    - [2] normalization and pre-processing
          + making samples comparable

    - [3] statistical modeling
          + linear models
          + p values

    - [4] statistical summarization
          + gene set enrichment
          + model checking

# I liked his example of live-coding a demo that was effectively designed to
# fail; very funny



*** Statistics is

    - [1] *statistics is the science of learning generalizable knowledge of data*

    - [2] *"statistics is study design"*

    - [3] *statistics is data visualization*

    - [4] *statistics is pre-processing*


*** Central Dogma of Statistics
# slide 34

    - [1] parameters :: aspects of the population
                        + we estimate this
                        + greek symbol

    - [2] data points :: values we measure
                         + capitalized
                         + subscripted (p_1, p_2, ... p_n)
                           - can also have two subscripts (e.g. x_1,4)

    - [3] estimate :: our guess at a population parameter
                      + this varies w/ ea. sample
                      + need to know what else could have happened w/ different
                        sample


*** Inference Vs. Prediction

    - [1] inference :: want to say something generally about the population
                       + prop. of people who carry a SNP

    - [2] prediction :: build a function to predict one property based on
                        measurement of another
                        + e.g., predict weight based on height

These two types of analyses have /distinctly/ different methods/designs/etc...

# different vs. predictive slide
# so, the top one is not predictive b/c the majority of points overlap
# also highlights statistical vs. practical signif. difference


--------------
/Test Metrics/
--------------
prob.(positive test | disease) = probability of positive test if you have
disease

etc... for the rest


-----
for the prediction problem slides:

key takeaway is you need to enrich for the rare outcome or you get a high number
of false positives



*** What is Data
"data are values of qualitative or quantitative variables, belonging to a set of
items."

set of items = often, from the population


-----
metzker ml - sequencing technologies - the next generation


-----
    - relativity of raw data :: everyone has their own raw data
	                        + i.e., my raw data is not the sequence images

"everyone has to keep track of what they did to the data" (UMGC, me, etc...)

"make big data as small as possible as fast as possible"



*** Parts of a Data Sharing Plan

    - [1] the raw data
          + can't touch this in any way

    - [2] a tidy data set
          + one variable per column
          + one observation in ea. row

    - [3] a code book
          + describes all vars. and their values
          + input = raw data
          + output = tidy data

    - [4] steps to go from 1-3 above

create scripts that re-create the work that you do.

# see the broman recommendations in the peerj slide (p. 82)


-----
steps to good file naming/organization

    - [1] slow down and make lots of notes
    - [2] have sympathy for your future self
    - [3] have a standard system that you understand



*** Genomic Data
three tables commonly associated w/ genomic data:

    - [1] genomic data
    - [2] phenotypic data
    - [3] feature data

# see the nature paper reference on slide 101




*** Variation
three sources:

    - [1] phenotypic variability
          + what you're actually interested in

    - [2] measurement error
          + technical replicates to address

    - [3] natural biological variation
          + clones to address

"technology doesn't eliminate biological variability"
# compare the variability seen w/ sequencing vs. array on slide 110
# read the paper too!

need many samples to properly estimate biological variability!


-----
    - sample size :: number of measurements you take



*** Statistical Power
check out the power.t.test function in R!

power considerations:

    - [1] sample size
    - [2] mean difference
    - [3] variability



*** Confounding, Batch Effects, and Randomization
literacy/shoe size example illustrates the effect of confounding variables
age affects BOTH literacy and shoe size

*batch effects are THE big confounder in genomics*

# the asian/caucasian example shows a histogram of p values

so, you need to measure stuff at the same time.

# read the batch effects article from nature reviews genetics


-----
randomization is a control for batch effects

# see the randomization approach on slide 153; shows randomization between AND
# within days AND sex


-----
a good study design:

    - [1] balanced
    - [2] replicated
    - [3] has controls
          + equal numbers in ea. group


*** Exploratory Data Analysis and Plot Design
slide 159 shows that best way to present bars right next to ea. other, not
stacked.


-----
graphs reveal structure that summaries don't
# see the regression plot example on slide 164
# see also the datasaurus dozen


-----
# ma plots vs. scatter plots -> see slide 172

# often, when a lab gets new equipment, you run a lot of technical replicates;
# later, when you're interested in variable, you do more biological replicates


*** My Data
I could do a lot of technical replicates to understand how much measurement
error there is in my data -> i.e., run the same sample on flow 10 times and look
at the result -> can do this for multiple reporters

* Friday, December 6 2019
** DONE Lecture: Transcriptomics/RNA-Seq
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_RNA-seq_001.pdf

10x as many splice isoforms as genes?

broadly, transcript isoforms are a major contributor to transcript diversity.
This includes alternative polyadenylation tagging, splicing, 5'-UTR diversity,
3'-UTR diversity, etc...


*** Major RNA-Seq Tasks

    - [1] discovering transcribed elements

    - [2] finding transcribed regions
          + this won't tell about splicing though

    - [3] assembling transcript isoforms
          + w/ or w/o a reference

    - [4] quantifying transcript abundance

    - [5] identifying DE transcripts

Can do total RNA preps, but rRNA will dominate these.


-----
steps for RNA purification

    - [1] isolate transcript RNA

    - [2] reverse transcribe to cDNA

    - [3] fragment cDNA

    - [4] size select fragmented DNA

    - [5] illumina sequencing of ea. end



*** RNA-seq Data Analysis
First step - do we:

    - [1] map reads to the genome
          + assemble aligned reads to ref. genome
    or

    - [2] assemble transcripts then align to genome
          + de novo alignment

One problem w/ alignment to a reference genome is that if you have, e.g.,
structural variation, your transcripts won't align to the reference genome.
But, this approach is more sensitive in most cases.


----------------------
/Aligning to a Genome/
----------------------
A major challenge to RNA-seq alignment is splice junctions.  Recall that the
splice junction doesn't actually exist in the referenece genome (see the green
bars on slide 19).


-----
slide 20:
alignment proceeds as follows:

    - [1] align reads that entirely align to an exon
          + initial alignment is w/ a tool that is not splice-aware, so
            reads pile up on exons in the genome (see slide 22)
          + you /INFER/ exons as those where reads pile up in the non-splice
            aware aligner (bowtie)
          + THEN - consider all possible splice junctions and infer splicing
            using a separate tool

    - [2] bring back the reads that didn't align and see if we can map them to a
      splice junction.  if a bunch of reads pile up at a junction, this means
      that it's a true splice junction.  note that this approach, in principal,
      would test all possible splice junctions.

    - [3] can even infer splicing from abundance of reads mapping to various
      exons.  so, if reads for exons 1 and 3, BUT NOT 2 are high, then 2 is
      probably spliced out (see slide 24)


-----
*do not use tophat says james taylor*


-----
"what is the smallest number of paths I need to completely cover the splice
isoforms I observe?"

this is the framework that cufflinks uses to identify overlap graphs to identify
splicing paths that lead to the isoforms we see.

in practice, this parimonious approach is not ideal and we actually see much
more diversity in reality.


-----
if you care more about isoforms than abundance, long-read sequencing is probably
the better approach.


*** Estimating Transcript Levels
we can tabulate abundance for:

    - [1] gene loci
    - [2] transcripts
    - [3] exons
    - [4] splice junctions


-----

    - [1] RPKM :: number of reads per kb of exonic bases per million reads in the library
                  + compensates for variable library size and transcript length

or

    - [1] TPM :: transcripts per million


-----
slide 38:

    - reads from first/last exons don't tell us anyting about splice isoform abundance
    - 75% of transcripts of this mRNA are isoform 1


-----
tools to use in 2019:

    - [1] HISAT
          + splice identification
    - [2] StringTie
          + quantification
          + reference-based
    - [3] Ballgown

for differential expression:

    - [1] cuffdiff works w/ StringTie
    - [2] sleuth works w/ kallisto
    - [3] DESeq2, limma also work

keys:

    - [1] enough replicates
    - [2] enough coverage

Need this to get sensitive, accurate measurement of your transcript of
interest.


-----
check out the paper on how many replicates are needed for RNA-seq differential
expression detection.

slide 49:
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_RNA-seq_001.pdf



work



** DONE Workshop: Statistics I
file+emacs:~/reader/cshl_computational_genomics/cshlcg-labs/cshl-lab1-2019.org
# see the org file here for the code

** DONE Lecture: Statistics II: Models, Experimental Design, Batch Effects
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_statistics_002.pdf

*** Clustering

    - [1] clustering :: find "close" samples/genes/etc..
                        + pattern-finding

clusetering involves defining the "distance" between features.

on slide 6, DC is Washington, DC and we find th distance betw. DC and
Baltimore.

the eqn. is A^2 + B^2 = C^2

The distance metric on slide 6 is euclidian distance on slide 6.


-----
then shows manhattan distance on slide 7.  note that the distance
traveled on ea. path is the same regardless of which path is taken.


------------------------
/Hierarhical Clustering/
------------------------

    - [1] find the two closest samples

    - [2] group them as one
          + multiple ways to do this
            - avg.
            - minimum
            - maximum
            - etc....

    - [3] repeat

The result is a dendrogram, where the result clusters down to two
branches.

distance on the dendrogram is the vertical length of the bars (but
this depends on which package you used to make the plot; can be
vertical only, horizontal only, or both).  think about this as the
distance you would have to travel on the lines on the graph to get
from one point to another.

slide 13: note how the dendrogram produces three clusters which agrees
well w/ the visualization on slide 11.


-----
*when doing distance mesaures, clustering, PCA, make sure to
normalize*



--------------------
/K-Means Clustering/
--------------------

    - [1] initialize cluster 'centers'
    - [2] assign values
    - [3] update centers
    - [4] reassign values
    - [5] repeat

so, the idea is that you keep moving the centers until you've
minimized the average distance of a center to ea. point in the
cluster.

*widely overutilized/overinterpreted*

this is apparently mostly done by PCA, which explains what the x/y
axis would be on ea. plot.


-----------------------------------------------------
/Principal Components and Single Value Decomposition/
-----------------------------------------------------
looking for multiple patterns in a dataset

three matrices:

    - [1] U matrix (samples x genes)
    - [2] D percent variation diagonal matrix
    - [3] Vt matrix describes patterns across genes


-----
these tell you what % of variance is due to a given pattern

see the toy example on slide 34.

so, the 'singular value'/'column' plots show the pattern on the x axis
and percent/proportion explained by it on the y axis.


-----
challenge w/ pca is when you have multiple patterns.

slide 36 shows a graph w/ 2 patterns, the 'block' (yellow/red) and
'stripe' patterns (the 'hi') in the graph.


-----
we use pca/svd to identify meaningful patterns and batch effects in
the data.


-----
slide 38 -> you would start w/ 3 clusters if you were doing k-means
clustering.


-----
slide 40 - pca_1 vs. pca_2

*read the paper this is based on*


-----
slide 41
tissue effect clusters more than the species effect


-----
*number of principal components = smallest number of dimensions*
*e.g., 50,000 SNPs, 2 treatments = 2 principal components*


-----
a common high variance explaining pc in genetic diversity studies is
geographic origin.



*** Preprocessing
goal is to remove technical artifacts.


-----
one confounder is GC content.

slide 48 shows the DE plotted against GC.  the key takeaway is the
most DE is a a result of differences in GC content.


-----
*see slide 53 for an approach to force distributions to agree*
*quantile normalization*


-----
#+NAME: key_point_bulk_effects
*make sure there aren't bulk differences betw. samples esp. due to technology*

if *every* gene is different in your sample or you have *GIANT* effects, you
likely have some kind of technological error.   this is what 'bulk effects'
refers to.


*** Regression
idea: minimize the sum of the squared errors of the best fit line
'find the the line the minimizes the total distance between all the points and
the line'

simplest approach: use the mean as the predictor.


-----
so, for this we use a standard line equation, BUT add a noise term.
(slide 73)


-----
    - residuals :: 'what's left after you fit the points to the lines'

always plot residuals after fitting to see if you missed some pattern.


-----
want a cloud of points if you're doing regression
(see slide 77 for when NOT to use it)


-----
can fit two lines based on some variable too (slide 83)
here, you add a 2nd slope and make it 1 for some value (males), and 0 for some
other class of values (females, in this case).


-----
common genomics approach: fit the same regression model to every feature (gene)
leads to hundreds/thousands of model fits



*** Inference
after we've done all of our work, we have millions of fits, results, etc..., but
how to know if we wouldn't get a diff. result w/ a diff. sample?


-----
general process is to fit a model where you fit based on what you care about
(treatment, e.g.) then take the variable you care about out and see if the model
is improved or made worse (slide 127).


-----
leek pushes the permutation approach, where you swap, e.g., treatment/control
labels randomly and generate an empirical null distribution.

nice slide on empirical null on slide 139.

shows the q value plot where any value for a p value is equally likely if
theres' no signal.  BUT, if there is a signal (signif. result), the p values
will pile up at low values around 0.

seems there's stuff on his blog about how you test models of signal vs. no
signal w/ p value distribution.


-----
see slide 161 for advice on how to do your analysis.

"the more futzing around you have to do, the more at risk you are of
p-hacking".

** DONE Workshop: Statistics II
file+emacs:/home/mahlon/reader/cshl_computational_genomics/cshlcg-labs/cshl-lab2-2019.org

** DONE Lecture: PSSMs and HMMs - Customized Scoring Matrices
file+emacs:/home/mahlon/reader/cshl_computational_genomics/practical_similarity_searching_003.pdf

# goals: improve search sensitivity for homologs

when you search w/ a single sequence, you will always miss homologs
searching w/ a model w/ multiple sequences will find more homologs

    - [1] pssm :: position-specific scoring matrix
                  + most-specific method for finding members of a protein family

    - [2] hmm :: hidden markov models

*hidden markov models are on the exam*



*** Protein Domains
colorization in the alignment w/ fastq, e.g., denotes that protein has a domain
that is annotated in pfam (a protein domain database).

what is a protein domain:

motif = 2-3 amino acids
domain = much bigger and can fold into something that is compact

so slide 2 is talking about the idea that domains are repetitive (chymotrypsin
has two repeated domains), but there are no proteins w/ just one of those
domains.

same is true for the purple barrel structure composed of multiple helices.

"the pieces don't exist by themselves"

for the immunoglobulin protein, this is an example of an evolutionarily mobile
domain, where different species have different numbers of these domains and we
find them in multiple proteins.

the immunoglobulin example shows a case where you look for homology at the level
of a domain.

"we find the same structure and sequence across evolutionary contexts"  -> this
is how we define domains.


-----
slide 3 shows how evolutionarily mobile domains crop up in unique contexts, so
you have both domains in the one alignment, but only part of the domain in the
bottom alignment



*** PSSMs and HMMs
reminder: pam and blosum are *positiion-independent matrices*

reminder: matrices that look back over a shorter time give more weight to good
alignments, those that look back over a longer time give fewer points for
alignment.  So, A-A for PAM40 = 8, A-A for PAM250 = 2

shallow = more sensitive and lets you see more recent evolutionary events (a key
point here is that w/ late-looking you see recent AND late events).


-----
slide 9 shows the time frame over which the matrices look over - can use this to
know which matrix you'll need to find homology over a certain time scale.


-----
slide 10 - the proteins have significant structural similarity but they DO NOT
have significant sequence homology

we use the PSI-BLAST program to find the homology.  more sensitive approach to
find homology.

works because position matters (we've been using position-independent).

as part of this process we build a matrix specific to the family of interest.


-----
slide 12 - this shows us doing pairwise alignment

but, when we go to the multiple alignment, things change:

note that when we bring in multiple sequences and lose alignment at the 2nd
alanine that was there in the pairwise.


-----
the matrix gets waaay bigger for position-specific matrix because you need 20
scores per column


-----
slide 15 - in the score calculation, we use a position as the 2nd probability.


-----
slide 18 - so on the 2nd round of model-building, you adjust the
position-specific scoring matrix to incorporate the things you found on the
first round.

a consequence of the above is the significant things on the first round get way
better on the 2nd iteration.


-----
slide 20
this shows the blosum62 matrix score for the various alignments

the rows are different positions in the protein


-----
so, on the first iteration, you just use a defined matrix


-----
PSI-BLAST down-weights things that are more similar to your starting sequence
because this increases sensitivity and allows you to find things that are
'farther away'.


-----
slide 21 - this shows how PSI-BLAST can lead to over-extension.

the idea is that you do the first alignment and it works well, then you make the
pssm.  the pssm gives you more 'wiggle' on the next alignment and you extend
over into the random sequence.

'the model gets 'contaminated''

note how the table shows that model contamination leads to BOTH a reduction in
true positives and an increase in false positives.


-----
slide 23 - this shows how pssms and hmms improve sensitivity.  so, the 2nd
iteration is when you add the pssm, note how the detection of true positives
massively spikes at iteration 2.

BUT, there's also a big increase in false positives w/ pssm.


-----
if your protein is a single domain, there can't be overextension.



*** Profile HMMs
we will make a hidden Markov model on the exam

HMMs are like PSSMs and are generally used to increase sensitivity/specificity.

HMM = model of things that happen by chance

HMMs have:

    - [1] states
          + foir coin, states are p(h) = 0.5, p(t) = 0.5

    - [2] transistions
          + so, you move to a different state here w/ diff. probabilities
          + non-fair coin in state 2

so, the progression through states isn't linear, you can be in state 1 for a
long time before getting to state 2.

the state sequence is hidden, though, and we have to infer it from the observed
symbol sequence (the sequence of heads or tails, e.g.)


-----
HMMs for proteins -> models that make a protein family

match states = amino acid column positions

aimno acid sequences get put w/ match states

in the slide if you're in match state one, you always add a C
if I'm in column 2, put out whatever amino acid (equally likely)
if I'm in column 3, put one of the 3 possibilities

if you've got 10 columns w/ data, you've got 10 aa's.  where data is sparse,
have deletion/gap states

i_o and i_3 are the other sides of the protein

the black bargraphs shows weights for ea. amino acid in ea. match state

slide 28


-----
slide 29

so, when there's no data, there's no match state

put the frequencies in the match states

calculate transition probabilities -> how many times to a go from match state 1
to 2.

the beginning and end states are really just there b/c they have to be, see this
from the fact that p is 1 for first and last transitions

an insertion state could put in multiple residues, but deletion can only take
out one.


-----
HMMs have position-specific gap penalties.


-----
slide 33 is showing that the pssms still miss homologs

the protein in the middle panel actually hsa the groes domain

bottom has both domains but both are missed


-----
slide 35

a key point is that once false positives come in, their friends come too!


-----
pssms improve phenotypic prediction


*no model gets all homologs*


-----
*match states are the columns in the multiple sequence alignment*

the model has to be able to account for any possibilities - so this is why you
have match, insert, and delete states at any point in the sequence

note that this means you also have to include all the possibilities in the
model, i.e., delete all, insert more than 1, etc... -> the model literally
allows you to calculate a probability for ANY outcome given the sequence you
started w/.

*** Multiple Sequence Alignment
*no mulitple alignments w/o homology*
*don't align things if they're not homologous*
*multiple sequence alignments ASSUME the aligned sequence ARE homologous*
*no statistical significance estimate from mulitple sequence alignments!!*
*i.e., things in multiple alignment are have significant homology*

# why multiple sequence alignments? resolve gaps


-----
progressive alignments work by iteratively adding sequences, but then you can't
fix the gaps.


-----
with multiple sequence alignment we're in definition of 2 of homology

it's about getting the columns right and this is about putting gaps in the right
place.  (i.e., the 50% homologous definition)


-----
star alignment - I think everything evolved at once and 2 changed to a C and 3
stayed the same.

muscle, mafft = SP-alignment


-----
muscle is good b/c it doesn't stick w/ the first alignment it did -> can get
progressively better.


-----
for the muscle comparison slide (53), bill says he doesn't care about the 0-20%
comparison b/c not sure if this is actually homologous or not, so if they're not
homologous, shouldn't be aligning.


-----
summary
'if your sequences are more than 50% identical, there shouldn't be that many
gaps' - so multiple sequence alignment should work well on these

** DONE Lecture: Sex Bias in Reference-Based Alignment
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_sex_bias_reference_alignment_2019_version.pdf

# important idea: putting research in a context that is testable (evolution!!)

-----
high pathogen load for humans in pre-industrial society

in today's society, very low pathogen load, so tuned up immune system that
evolved in high pathogen load is overactive, so cancer/immune disease.


-----
there is no population that is ancestral to another
BUT there are populations that live like pre-industrial humans.


-----
in XX individuals, one chromosome is 'turned off'

X inactivation apparently happened gene by gene

some immune-related genes escape X-inactivation, hence the hypothesis for X
chromosome basis for increased autoimmune disease in women.


-----
you will align reads from the X to the Y.  This is why you mask out the Y.

** DONE Workshop: Multiple Sequence Alignment/PSSMs/HMMs
*** #1
:honeybee_GST:
>NP_001171499.1 glutathione S-transferase D1 [Apis mellifera]
MPIDFYQLPGSPPCRAVALTAAALDIEMNFKQVNLMNGEHLKPEFLKINPQHTIPTIDDNGFRLWESRAI
MTYLADQYGKNDTLYPKDLKKRAIVNQRLYFDMCSLYKSFMDYYYPIIFMKAVKDQAKYENIGTALSFLD
KFLEGENYVAGKNMTLADLSIVSTVSTLEALDYDLSKYKNVTRWFAKIKPEIPKYEEYNNAGLKMFKELV
NEKLSKK
:END:

look at what happens to stringent starvation protein

why does the alignment get longer

blosum50 alignment for this protein = ~150

blosum 62 is shallower, so it doesn't allow as much alignment

so, the alignment improves b/c you're using a different matrix


-----
so, the idea is that if you can see all the 'true' homology, the scores should
be high and the alignment should extend over the homologous domains.


-----
with the pssm, we're no longer searching w/ the honeybee protein


-----
so why do you pick up a 1512 syep human protein in iteration 3, when GSTs are
like 200 amino acids long?

he gets included in the model after the first time we pick him up.  NOW, his
information is included in the matrix and this creates problems.

syep goes from like 0.001 to 3e-20!!!!


-----
*be sure to look at the %identities*
*something like 16% identity means identity TO THE MODEL, not the bee protein*


-----
highest scoring non-homolog is XENLA.  non-homolog b/c no domain and not
significant.

a key point here is that proteins like this still get low scores, suggesting the
model still works even after we've established the pssm.

*expect highest scoring non-homolog to be at 1*


*** # 2
QFO78 = database for 78 eukaryotic complete proteomes

putative non-homolog = no pink domain -> a positive signal (BUT NOT DEFINITIVE)
that you may not be related

the yellow domain that comes up could be something unrelated (separate domain)


-----
why haven't these proteins been annotated as having the domains even though we
see homology?  They're too far away and we don't get them w/ most searches.
it's hard to find.


*** #4
40 letters across the top
first 20 = score
second 20 = frequency

there are 218 rows in the matrix; don't include gapped sequence
this is how many posns. we can calculate a model on

why not 75:25?  sequences get weighted differently based on how different they
are from ea. other.

note that the M-A alignment score differs based on position (POSITION-SPECIFIC)


-----
the hmm also has 218 states


-----
for the hmm, smallest number = highest probability (cause of logs)

first row = match state
second row = insert state
third row = transition state

m -> m = match to match
m -> i = match to insert
m -> d = match to delete
etc...



*** Understanding the HMM

There are three rows for ea. position on the HMM:

    - [1] the match probability
          + top row
          + 20 amino acids across the row

    - [2] the insert probability
          + 2nd row
          + 20 amino acids across the row

    - [3] the transition probability
          + 7 possible transition states
          + m->m, m->i,m->d, etc....

each of the numbers corresponds to a probaility, but these are transformed such
that a low value corresponds to a higher probability (e.g., e raised to low
probability = smaller number than e raised to high probability).  so, position 3
w/ the following:

https://fasta.bioch.virginia.edu/fasta_www2/chaps.cgi?msa_query=gstm1_human%0Agstm2_human%0Agstm3_human%0Agstm1_mouse

has nothing but M in the alignment.  in the hmm output, this has the lowest
value (1.48).  Leucine has a close value to this (1.79), but this is a close
transition in terms of mutation/aa properties.

note that you see a similar effect in the pssm:
posn. 3 alignment score = 6 for M, 2 for L, negative for most other things


-----
*the insertion numbers are based on rel. frequency of amino acids, so don't
really mean anything*


-----
there aren't any gaps in the alignment, so you won't see a high probability of
insertion/deleiton on the hmm.

* Saturday, December 7 2019
** DONE Lecture: Short Read Alignment
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_Short_Read_Alignment_2019_001.pdf

short read utility:

    - [1] quantitation given a reference genome

    - [2] re-sequencing a reference

    - [3] pooled samples
          + i.e., fish out proportion of virus 1, virus 2, etc... in a pool

idea here seems to be that short read sequencing is good when you have a
reference genome.


-----
for alignments, you 'hash' the target sequence for every k-mer and find all
possible k-mer matches in the genome.

steps:

    - [1] start w/ a 10 mer - find all possible matches

    - [2] go to 11 mer - find all possible matches

    - [3] go to 12 mer, etc...

    - [4] BUT, needs some wiggle room for sequencing error/variation

The hash table approach is efficient b/c it indexes every k-mer as an integer.
so, you look up the integer, not the sequence.


------
/Trie/
------
a different way of mapping than hash table:

ATGATCA
ATGCATA
ATGCAGA


 |
 A
 |
 G
 |
 T
 |
 -
| |
A C
  |
  -
 | |
 A G

etc...

so, you walk the graph ea. time to get your sequence and store a value for a
given sequence at ea. node.  so, there would be integer values at the nodes in
this graph.

basically, this and the hash table are a way to index sequences into memory.


-----
hash table illustrated:
|---------+-------|
| key     | value |
|---------+-------|
| ATGA    | V1    |
| ATGATCA | V2    |
| ATGCATA | V3    |
| ATGCAGA | V4    |
|---------+-------|

"*the key has to be a string*"


-----
the banana slide/suffix tree (10):

|--------+-------|
| key    | value |
|--------+-------|
| BANANA |     0 |
| ANANA  |     1 |
| NANA   |     2 |
| ANA    |     3 |
| NA     |     4 |
| A      |     5 |
|--------+-------|

every prefix is contained in at least one suffix in the table.

"the value is is the position where that suffix occurs"

"it's a suffix table, so you start from the end - i.e., biggest suffix goes
first."

"looking for prefixes in suffixes"


-----
"we could build a suffix tree for an entire chromosome and walk down the suffix
tree and find alignment w/ our RNA seq read (e.g.) and find the position on the
genome that it matches"


-----
genome can be stored in "two bit" format:

00 A
01 C
10 G
11 T

still, building the hash table/suffix tree for a genome the size of a human
leads to a massive memory problem (slide 12)


-----
burrows-wheeler transform permutes characters to create runs of identical
chars.  the compression comes from turning, e.g.:

"rrrrrrrrrr" to "10r"

data becomes more "squishable"

so, MOST compressible format is just base frequencies:

store the genome as: "100A, 100C, 100G, 100T", e.g.

a key property of the burrows-wheeler transform is that we can reverse the
compressed string back to the original order.


-----
bw transform works by "pushing" the first character onto the end, so

*A*CGAGC$ becomes
CGAGC$*A*

next step:

GAGC$*A*C

etc...

second step is lexicographical sorting (i.e., alphabetical, but "$" comes
first).


-----
"number A's C's G's and T's is the same in very column in the BWT matrix"


-----
the pileup of runs is vertical in the BWT string (slide 18)


-----
slide 31

top row is read; matrix is genome we're aligning to

so the alignment works by doing last-first mapping with ALL possible matches;

see how it's applied to multiple matches.  we keep going w/ this approach until
we run out of matches

so, the slide shows the alignment narrowing down to a SINGLE match; the read
itself is 'AAC' and we align it to THE ONE match that it hits w/ in the matrix.
notice how it proceeds in reverse to get the alignment to the correct sequence.

*THE READ IS THE TOP 'AAC' AND THE GNEOME IS THE MATRIX*


-----
we have some room for errors by 'walking back' if we have no alignment and
exploring options where we have mismatches.


-----
bwa-mem is standard for variant calling (slide 35)


*** Genome Variation
goals:

    - [1] find SNPs
          + polymorphism when it has appreciable freq. (e.g., 0.1%)
          + could also include a few adjacent variants
          + need sequencing to detect
            - have to compare the variant organism to reference
            - thus, need multiple observations

    - [2]


-----
slide 45:

This is about determining whether sequence differences you see are true SNPs.
You get multiple reads of the same locus and can use this information to decided
about whether you have a SNP or not.  Also, not that the position is ONLY c or
t and they occur in equal frequency (1:1).  This would be consistent w/ a
heterozygous variant.

For other positions, you may have a variant, but cannot say definitively.
usually want about 50-60x coverage to confidently call a SNP.


-----
*SNP detection one of the easier detection problems w/ sequencing*

but, need to account for sequencing errors!!!


*** Genome Analysis Toolkit GATK
*best kit for variant analysis is 'genome analysis toolkit (GATK)'*
# don't use anything else.

https://samtools.github.io/hts-specs/
# specs on variants

https://software.broadinstitute.org/gatk/
# GATK website

** DONE Workshop: Transcriptomics with Galaxy (De Novo Assembly with Reference)
*throw away unpaired reads!!* - *want the paired reads*

Question 1: read length = 99 bp
Question 2: read quality declines to unacceptable quality at the end of the read
G and C have overall higher quality

the answers to the above questions are consistent over the forward and reverse
reads and are consistent betw. replicates.


-----
Question 3: trimmomatic removes 2-3 bases from the end.
the best scores are achieved in the middle of the read, forming an inverted "U"
shape.


-----
so, you get unpaired reads from trimmomatic b/c if one read is good and the
other is bad, you throw out the bad and keep the good.  now, the good read has
no pair.  this is what goes into the unpaired reads file.

*only unpaired reads go into the unpaired reads file*


-----
workflow for this lab:

    - [1] raw reads; per sample:
          + R1 - forward read
          + R2 - reverse read

    - [2] trimmomatic
          + remove bad reads
            - get paired reads (YOU WANT THESE)
            - get unpaired reads (DON'T USE)

    - [3] hisat2
          + construct a transcriptome
          + identify all the transcripts in your sample

    - [4] stringtie
          + assemble a transcriptome
          + gtf file representing transcriptome (gene calls)

    - [5] stringtie merge
          + add a reference genome

    - [6] featurecounts
          + look at counts to get DE

** DONE Lecture: Genome-Wide Profiling of the Chromatin Landscape

    - [1] chromatin :: "substance in the cell nucleus wich is readily stained"
                       + old definition


-----
# goals

    - [1] what is the chromatin landscape
          + nucleosomes
          + RNA polymerase
          + TFs
          + DNA
          + chromatin remodelers
          + DNA methylation
          + histone modification
          + histones
    - [2] how do we map chromatin landscape?
    - [3] regulatory element dynamics
    - [4] single-cell chromatin profiling

-----
-----------------------
/Nucleosome Remodeling/
-----------------------
3 types:

    - [1] iswi/chd
    - [2] swi/snf
    - [3] ino80

check out kubik et al., nsmb 2019 for more information on how remodeling works


-----
occupancy vs. positioning:

for a locus in a population:

    - [1] occupancy :: % of loci w/ a nucleosome

    - [2] positioning :: spatial location/precision of nucleosome position


-----
many methods of chromatin mapping rely on using an antibody to enrich for
chromatin-specific DNA (e.g., CHiP-seq)


-----
------------------------
/Enzymatic Footprinting/
------------------------

    - [1] mnase preparation

uses micrococcal nuclease -> this enzyme has exo- and endonuclease activity.

the only thing mnase can't chew is DNA bound to protein, so you get all
protein-bound DNA through this.

    - [1] DNase prepation

same idea

    - [1] tn5 tagmentation

dump the adapters into any place in the genome where the tagmentation enzyme can
fire - so, won't fire into chromatin/tf-bound DNA - infer chromatin from absence
of DNA in the readout.  allows direct sequencing w/o adapter ligation.


-----
limitations of existing CHIP-seq:

    - [1] low efficiency (millions of cells needed)
    - [2] high background (10's of millions of reads)
    - [3] low resolution (hundreds of bp)
    - [4] most antibodies fail
    - [5] artefacts



*** Chromatin Profiling By Enzyme Tethering
CUT&RUN:

    - [1] fix TF's to DNA
    - [2] add in antibody to TF
    - [3] add MNase tethered to protein A
          + the protein A binds the antibody
    - [4] TF-antibody-DNA complex diffuses out
    - [5] sequence what's left
          + i.e., nucleosomes
          + need to do paired end sequencing



*** How Do TFs Fin Their Sites in Chromatin
possibilities:

    - [1] transient unwrapping of TF binding sites

    - [2] 'pioneering' activity
          + TF sits on nucleosome even though it can't activate transcription
            until the nucleosome moves

idea: w/ pioneering transcript, the mnase can cut the nucleosome out, so you'd
see bigger fragments w/ it.

** DONE Lecture: Accessing Public Sequence Databases
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_Public_Databases_001.pdf

# goal: public datasets and where to get them

UCSC has lots of data mapped on to the genome you're looking at.

GEO = minimally processedata
SRA = short-read sequencing archive

EBI is easier to get data from than SRA (these sites mirror ea. other, so the
same data should be found on both sites).


-----
reference genome occurs in '.fa' format and this is a single file that will
contain the entire genome sequence across a single FASTA entry.

slide 3: note the distinction betw. 'canonical' and 'full' genome.  This is
probably what you want for your work.


-----
-------------------------
/Gene Annotation Formats/
-------------------------
GTF = gene annotation files

BED = another annotation file type; apparently more flexible than GTF.


-----
slide 5
emphasizes that the most important thing you need is a table of numbers

tables of numbers:

    - [1] count data for RNA seq
    - [2] TPMs/FPKMs
    - [3] coverage of ChIP peaks
    - [4] variant counts?

apparently it's possible to combine datasets more commonly than you'd think.




*** UCSC
A major of advantage of this resource is the many tracks that have been overlaid
on the information on this database.  The idea is that you can visualize all the
information in a 'track' (say, methylation status) on the whole genome or in
very localized regions.

Two key features:

    - [1] table browser
          + grab stuff from the tracks you're looking at

    - [2] downloads
          + e.g., a reference genome

Scroll to the bottom to get more information on what the track you're looking at
is.



*** ENSEMBL
An altenative to ensembl.

An advantage over UCSC is the ability to more rapidly download information; I
think I remember this from the bioinformatics book.


*** UCSC Xena
Combined data hub where you can download pre-harmonized data sets.  Includes
data from a lot of repositories.


*** Refine.Bio
attempts to get gene expression levels for all of GEO/SRA via a standardized
methods.  Better way to get expression from these resources.

https://www.refine.bio
[[https://www.refine.bio/search?q=yeast]] e.g.

* Sunday, December 7 2019
** DONE Lecture: Mid-Term Exam Review

# 1. (a) Highest scoring non-homolog tells us what non-similarity looks like 

# (b) What makes you think something is a non-homology?  non-signif. E value.
# but, we don't know if it's truly a non-homolog, so we also have to do another
# search, look in another database, etc...  recall the example of the first lab,
# where the first time we called highest scoring non-homology, but later in the
# week found it to to be a homology w/ different parameters, database, etc...

# 2. use a more shallow matrix so you don't look so far back in evolutionary
# time.  using the shallower scoring matrix makes sense here if the true length
# is 50 aa b/c you get more bits per aa.  (a) gets better, (b) gets worse

# 3. (a) more chances to get a good score by chance; if I had 10 coins, the odds
# of getting 10 heads in a row goes up if I flip more often
#    (b) smaller; 
#    (c) no; ability to detect homology is affected by db size

# 4. (a) do another search with just that domain pairwise - id homology there, then
# you know you have homology.

#    (b) too evolutionarily distant

# 5. for the traceback, all you have to do is go through the paths that give you
# the highest score at each node, so if there's 2 zeroes, you have to trace both
# of those paths back.  You follow the path that gets you to the best score.
# HAVE TO START AT THE END

# 6. similarity in bit scores:
# e^λs = q_i,j/p_i,p_j (re-arranged equation)

# K = length correction
# side note: 1 - e^-x, when x < 0.1 = e^-x
# sequence is too short to get signif. homology
# E value 84 means better alignment than 84% of the sequences

# for the de bruin graph, the fact that we don't have the 5 mer is why we can't
# loop back at the branch point

# can't reconstruct the original sequence b/c can't resolve the number of
# repeats.
# if this were a real assembly, we'd have 4 contigs
# YOU CAN'T RESOLVE REPEATS THAT ARE LONGER THAN YOUR READ LENGTH


-----
# statistics
# BLAST = prediction, but also a prediction?  




#+BEGIN_SRC R
(5 * 0.1915) = log()

(5 * 0.1915) = 0.9575 

0.9575 = exp()

aa_match <- exp(0.1915*5)
aa_mis <- exp(-4 * 0.1915)
aa_mis * (0.25)^2

(aa_match * (0.25^2)) * 4

(16 * 5) - 16

((0.1915 * 64) - (log(0.177)))/(log(2))

((0.1915 * 64)) - (log(0.177 * 20 * 20))
#+END_SRC

*** BWT String Reversal
|---+---+----|
| $ | T |  1 |
| A | T |  7 |
| A | $ | 21 |
| A | A |  6 |
| C | A | 20 |
| C | T | 15 |
| C | C | 19 |
| C | C | 14 |
| C | C | 18 |
| C | C | 13 |
| C | T |  3 |
| C | C | 17 |
| G | C | 12 |
| G | T |  9 |
| G | G | 11 |
| T | C |  2 |
| T | G |  8 |
| T | C | 16 |
| T | T |  4 |
| T | G | 10 |
| T | A |  5 |
|---+---+----|

*** HMM Frequency Calculation
|---+------+-------+------+------|
|   | c    | c     | -    | g    |
|   | c    | c     | -    | g    |
|   | c    | g     | -    | g    |
|   | c    | t     | a    | g    |
|   | g    | t     | a    | c    |
|   | c    | t     | a    | g    |
|   | c    | t     | a    | g    |
|   | g    | c     | g    | c    |
|   | g    | a     | t    | c    |
|   | g    | a     | t    | c    |
|   | g    | a     | t    | c    |
|   | g    | a     | t    | c    |
|---+------+-------+------+------|
|   |      |       |      |      |
|---+------+-------+------+------|
| a | 0    | 0.33  | 0.44 | 0    |
| c | 0.50 | 0.25  | 0    | 0.50 |
| g | 0.50 | 0.083 | 0.11 | 0.50 |
| t | 0    | 0.33  | 0.44 | 0    |
| - | 0    | 0     | 0.25 | 0    |
|---+------+-------+------+------|
#+TBLFM: @15$3=3/12

4/9
1/9
0.33 + 0.25 + 0.083 + 0.33

100/1e6

0.05 * 4949
15000 * 0.05

5591-750

0.75 * 5591



#+BEGIN_SRC R
qnorm(p = 0.00001, mean = 30, sd = 5.5, lower.tail = T)
pnorm(q = 5, mean = 30, sd = 5.5, lower.tail = T)
#+END_SRC

** DONE Lecture: Regulatory Genomics
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_Regulatory_Genomics_001.pdf

# goal: understand gene regulation on a genomic scale

waddington's epigenetic landscape: shows how starting from a pluripotent state
can lead to you taking on a new epigenetic state during differentiation.


-----
So, how do we get gradients of expression during development.  You need
gradients to get cell differentiation/morphological development.  Regulatory
genomic elements apparently do this, focus in this talk is on enhancers.
dramatic example: mutation in an shh enhancer leads to polydactyly.

"if you have a mutation in coding sequence in shh, probably lethal.  BUT, you
can have mutations in regulatory elements that lead to less severe (but
evident) phenotypes."




-----
Key concept: you have multiple regulatory elements for genes (e.g., shh) and
utilization of these elements varies across tissue/cell-types.  This leads to
different levels of shh based on alternative regulatory element usage.


-----
"enhanceropathies"



*** How to Discover cis-Regulatory Elements in the Genome
key concept: we are looking for motifs


-----
slide 15: shows various types of TF binding motifs

a problem w/ motif identification is that they are small, e.g., 'CCAT' element
for YY1 TF is 'CCAT', this occurs w/ (1/4)^4 = 3.906e-3
                                       1/256 = 3.906e-3 


-----
how to identify true motifs w/ very short sequences?

*look at conservation!!!!*
slide 18 shows this for 481 (non-coding putative regulatory) elements with >
200bp sequence with 100% sequence ID betw. mouse and rat. 

steps:

    - [1] look at non-coding regions

    - [2] look for high levels of conservation

    - [3] look for regulatory effects
          + add element in
          + delete regulatory elements
            - deletion does surprisingly little (slide 20)


-----
goal of ENCODE was to identify all regulatory DNA elements. 

some types of elements ID'd by ENCODE:

    - [1] enhancers
    - [2] promoters
    - [3] repressors
    - [4] silencers
    - [5] insulators

for these analyses: get expression by RNA-seq

"when TF's are bound to the genome, these are candidate cis-regulatory
elements". 


-----
Looking at ATAC-seq data gives you open chromatin locations.  Can use this to
infer regulatory elements (which are those that are outside of the gene body).
a big peak in a non-coding region would suggest a potential regulatory element,
esp. when you have high expression, I think.  Ultimately, you're going to
integrate various types of chromatin-seq data to get a full picture of how the
chromatin landscape looks. 


-----
*DNA methylation can be a repressive modification*
*genes that are transcribed get methylated in the gene body*
*regions that are unmethylated tend to be regions of open chromatin*



*** Connecting Distal Elements to Their Target Genes

    - key concept :: regulatory elements are often far from their target genes


-----
HI-C = 'hybridization capture'

use probes to enrich for specific parts of the genome (e.g., target promoters).
pull out from HI-C only things that are interacting w/ promoters. 

(check slide 37 to see what this looks like in data). 


-----
note that alternative promoters are also a form of regulatory element. 


-----
insulators now = 'architectural element' (insulators don't always insulate,
apparently).


-----
vast majority of enhancers are in intronic regions or in promoter regions

** DONE Lecture: Genomics of Gene Regulation 1: Analyzing Protein-DNA Binding Interactions
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_Protein-DNA_Interactions_001.pdf

# goal: using machine learning techniques to understand gene expression
# why study protein-DNA interactions


-----
    - question :: how do we think about sequence features in the genome?
		  why are we interested in protein-DNA interactions in the
                  nucleus?


-----
why study protein-DNA interactions:

    - [1] regulation of many genes occurs via this mechanism
          + many of these can be probed by the various chip/atac-seq methods


-----
Focus of this talk is sequence-specific tf's (i.e., those w/ a known/specific
motif).  These can regulate:

    - [1] initiation of transcription

    - [2] transcription elongation


-----
so, the idea is that tf's are incredibly important for a host of phenotypes.
ex: you can add tf's to get cellular reprogramming (yamanaka stem cell
reprogramming [nobel prize!])




*** Computational Prediction of TF Binding Sites
TF binding motifs are typically:

    - [1] short :: typically 6-20 bp long

    - [2] degenerate :: have 'preferred sequqnce', but no exact match required
                        + "they're not like restriction enzymes in that sense"
                        + motifs have similar preferences across protein families
                          - so, ZFs bind similar motifs, e.g., 


-----
so, key questions:

    - [1] how do TFs find their preferred sequences?
    - [2] why do they bind to their preferred sequence

start by representing binding preferences as a multiple sequence alignment.
represent this as a multiple sequence alignment.  the main approach for this is
the 'weight matrix' that outputs a probabilistic view of what a TF binds
to. (slide 15).

--------------
/Count Matrix/
--------------
Make a frequency matrix for the binding preferences.  convert into a relative
frequency matrix.  in the initial representation, the count frequency matrix is
*position-independent*, but this probably doesn't conform to reality (ignores
possibility of sequence effects). 


-----
so, the 'sequence logo' is letters (x axis) and bits (y axis).  nucleic acids
are represented in 2 bits:

    A - 00
    C - 01
    G - 10
    T - 11

So, you have 2 bits of information if, e.g., only 'G' at a posn., less than that
for anything other than certainty at a position. 

so, if it was A/G in the sequence logo, you know '1 bit' of information (in the
table above, you know last number is 0), so you have basically 1 bit of
information. 


-----
in a position weight matrix, you divide every position by the rel. frequency of
the associated nucleotide.  


-----
we use this matrix to test the hypothesis:

    'we have some sequence, what is the probability it is an NFKB site?'
    # null hypothesis is not an NFKB site

take the position weight matrix for a TF binding site motif, then log transform
it to get a probabiliyt matrix

for nfkb sequence:

p() nfkb = 0.97 * 0.97 .....
p()!nkfb = 0.25 * 0.25 .....

now, divide the two probabilities to get an odds ratio. 

low score on the odds ratio = not an nfkb site
vice versa high score 


-----
motif scanning tools
(slide 19)



*** How Do TFs Bind Their Targets

Sequence preference alone does ont fully predict TF binding in vivo. 

(slide 26 shows all the motifs and the blue shows that only some of them are
bound at at a given time) 

so, why are the sites not all bound?

    - [1] chromatin
    - [2] cofactors
    - [3] methylation/other sequence modifications 

(slides 29-30 shows this well)


-----
so CHIP-exo gets higher resolution because it uses the exonuclease to shorten
the amount of DNA to only what's around the protein itself, /everything/ else
gets digested. 

 

*** CHIP-Seq Data Analysis

    - [1] align reads (DNA that was bound to TF) to genome
          + visualize where our TF binds in the genome 
          + also get strandedness from this analysis
          + all the reads 'point' to where they're bound

    - [2] pseudo-extend 5' ends of reads
          + done via 'data smoothing'
          + I think this is smoothing the two strands together to a single 'peak'
          + if you don't have data from both strands, the tools will throw it out

    - [3] can make peak alignments and view them w/ overlaid peaks in ucsc

    - [4] can also do heatmap


-----
chip-seq data can be broad (histone mark) or very sharp (tf binding site)

this means that the tools you use to analyze the data will vary depending on
what you're looking at. 

    - key task :: discern signal (stat. signif. signal) from noise in the plot
		  + this varies based on what type of signal/mark/binding you're
                    looking at
		  + *controls very important!!!*

can use igg as control antibody.  

"always have to think about practical vs. statistical significance"


-----
next steps after id'ing peaks:

    - [1] functional analysis
          + coding/non-coding
          + pathway analysis

    - [2] molecular verification 

** DONE Workshop: ChIP-Seq in Galaxy
https://shaunmahony.github.io/chip-seq-tutorial/
* Monday, December 8 2019
** DONE Lecture: Genomics of Gene Regulation 2: Characterizing Transcription Factor Binding Dynamics

file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_Protein-DNA_Interactions_002.pdf

# what do we do w/ a set of chip-seq peaks? 

we've found:

    - [1] tf chip-seq peaks
    - [2] histone modification domains
    - [3] atac-seq domains
    - [4] etc...

in multiple cell types. 

shaun recommends against the venn diagram approach (i.e., don't say we found
peaks in both conditions, those are the ones that matter).


-----
slide 4 - this illustrates 'overdispersion' - greater variance in replicates
than that expected by sampling process

he's emphasiszing that the venn diagram approach loses interesting information
about differences betw. samples.  so, in the blue 40% plots, you have real
differences in your top left quadrant, but you say they're the same. 


-----
*instead* 

treat chip-seq data as quantitative data (akin to RNA-seq).  treat is just like
any other quantitative data in biology. 

see slide 13


*** Key Point
*KEEP IN MIND THAT NOTHING MAGICAL HAPPENS WHEN YOU APPLY A BINARIZING THRESHOLD*
"we don't treat genes in RNA seq as if they're expressed or not expressed, so
why do the same for chip-seq peaks?"



*** Characterizing Sequence and Chromatin Determinants of TF Binding
# key question: how do TFs bind and recognize their target sequences:

    - [1] find accessible domain
    - [2] work w/ co-factor that opens chromatin for TF to bind

using inducible mouse stem cell lines that become neurons when TF is
overexpressed. 

slide 25: many genes become differentially expressed after inducible TF
induction - becomes more pronounced w/ time. 

# key point: the de genes are neuron specific


-----
looking at the plot of shared de genes (slide 32) - note that many more genes
are shared de betw. the two than are unique -  *so*, expect that there are many
shared peaks betw. the two.  

*instead*, see many differentially bound sites and fewer shared sites. 

trace this to difference in motifs (slide 34). *and* when the sites are shared,
they both actually have their preferred k-mer (!!!) (slide 36).



*** Convolutional Neural Networks
used commonly for image analysis.

patrick is using DNA sequence as a type of 'image'.  instead of a three-channel
RGB image, he treats the DNA sequence as a four channel 'image' that has only
binary values for ea. sequence position:

|----+---+---+---+---|
| NT | 1 | 2 | 3 | 4 |
|----+---+---+---+---|
| a  | 1 | 0 | 0 | 0 |
| c  | 0 | 0 | 0 | 0 |
| g  | 0 | 0 | 1 | 1 |
| t  | 0 | 1 | 0 | 0 |
|----+---+---+---+---|

use this to build the CNN.  you then give the cnn the regions of interest to
compare against the entire genome. 

# goal: predict whether a specific region is bound/unbound on activation of a TF

one insight is that having more of a certain motif in a region is predictive of
more binding (this might be proof of principle). 


-----
slide 53 - the light pink shows regions that are inaccesible (atac-seq).  
ascl1 and neurog2 are binding these prev. unopen sites ('pioneering'). 

this means these proteins are changing the chromatin landscape. 

** DONE Workshop: CHiP Peak Annotation in Galaxy
https://github.com/pdeford/cshl-2018/blob/master/peak-annotation-galaxy.md
** DONE Lecture: Chromatin States 1: Analysis of Histone Modifications
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_ChIP-seq_histone_modifications_001.pdf

# goal: understand how chip-seq for TFs can provide information on expression
# regulation.  

# also: how are patterns of histone modifications useful for understanding gene
# expression regulation. 


-----
histone modifications provide information on geonmics annotation as well as
mechanisms of regulation


-----
-------------------
/Histone Structure/
-------------------
147 bp wrap around the nucleosome

tail is called the 'tail' b/c it's what hangs off the core of the histone *not*
b/c it's the C-terminal tail -> this is where the business happens w/r/t to
histone regulation. 


---------------------------
/Modification Nomenclature/
---------------------------
histone: H1-4
AA: R, K, S, T
AA pos.: #4
modification: acetylation = ac, methylation = me
n. methyls = me1, me2, or me3 (i.e., me3 =  3 methyl groups on side chain)

so, histone H3 lysine 4 trimethylation = H3K4me3


-----
how to choose a modification to choose:

    - [1] hsk4me3
          + marks polymerase II bound promoters
            - not necessarily active promoters
            - but not repressive either
          * polymerase ii does mRNA, LncRNA, and miRNA  

    - [2] h3k79me2
          + marks transcribed regions 
          + mark occurs towards 3' end of genes 
          + see slide 14 for how this co-occurs w/ hsk4me3

    - [3] h3k4me1
          + marks enhancers used by the cell
          + not necessarily active promoters
          + not much of this in yeast
            - enhancer poor species

    - [4] h3k27ac
          + marks promoters and active enhancer elements
          + h3kme1 w/o h3k27ac = 'poised enhancer'
          + see slide 20 for how frequently the above DO occur together

    - [5] h3k27me
          + broadly repressive modification


-----
ENCODE has 'blacklisted' intervals that you remove from your data, because they
commonly correspond to spurious peak regions. 



*** Peak Calling
most peak finding is done w/ MACS2. 

Questions:

    - [1] should you merge replicates then call peaks?

    - [2] should you call peaks per replicate, then use only overlapping? 

    - [3] should you use 2 peakfinders to call 'true' peaks


-----
Quality metric for ChIP-seq: IDR = irreproducible discovery rate - Measures consistency between replicates in high-throughput experiments
 

-----
chip-seq training resources: slide 33


*** ATAC-Seq
https://www.activemotif.com/blog-atac-seq

Adds adapters via Tn5 transposase.  The transposase breaks /primarily/ open
chromatin.  you amplify the stuff contained in open regions *BUT* this includes
the pieces that hang off of nucleosomes.  

basic idea is to get TF binding regions/open chromatin in high-throughput
format. 

Integrate this w/ histone modification data to see where things are binding. 

can get information about nucleosome positioning via this approach. 


-----
slide 44 has ATAC-seq tutorials

encode project site also has qc standards (slide 45). 


-----
** DONE Lecture: Gene Lists to Pathways
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_Pathway_Analysis_001.pdf
[[REVIGO][Preferred_GO_Tools]]
[[https://bioinfogp.cnb.csic.es/tools/venny/][VENN_diagram_tool]]

# key concept: gene sets are not pathways 
# gene set = genes that do the same thing
# pathway = set of genes that serve some function

# goal: extract information from significant gene list/set
*these are hypothesis-generating tools -> use them to develop hypotheses that*
*you would then test about how pathways are influencing your phenotype*


IPA = manually hand-curated information (requires paid subscription)
GoRilla = possible GO annotation tool

-----
three types of tests:

    - [1] over-representation analysis
          + do I have more 'things' than expected by chance

    - [2] functional class scoring 
          + uses quantitative information as well

    - [3] pathway topology
          + looks within a pathway

maybe look at the paper that slide 4 references. 


-----
in GO analysis, labels are not independent of ea. other, which is why you get
blocks of things w/ similar annotation/p values. 


-----
preferred GO tools:

    - [1] BOrilla
    - [2] REVIGO
    - [3] ToppGene
    - [4] GREAT
          + primarily for regulatory elements



*** Gene Set Enrichment Analysis
permutation-based method to determine whether your genes in a pathway are
randomly distributed throughout a list or primarily at the top or bottom.  

more sensitive than GO-based analysis. 

see slide 14 to see how signif. distributions looks

GSEA is a downloadable tool. 


*gene sets are not pathways* *pathways have directionality - they have
nodes/edges*

examine slide 16 which contrasts pathways and gene sets

so, the pathway topology presumably gives you more/better information. 


-----
most stat tests are predicated on the idea of independence of observations BUT
gene sets are not independent.  

there are tools to help with this issue. 


-------
/WGCNA/
-------
groups genes together by correlation. 

useful for pathway discovery - so, identify pathway components by looking for
signif. co-expression of genes. 


*** Controls
what is the right control for these analyses? 

for cancer -> take a related, BUT different, cancer

remember to use the right background set for these analyses 

also, can you find a positive control? 
 
** DONE Lecture: Single Cell RNA Seq
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_Single_Cell_RNA-seq_001.pdf
[[ss-seq][single-cell seq tool recommendations]]

Most useful experiments of this type will have thousands of cells or more. 

Most single cell setups get only 3' end data, i.e., you only sequence from one
end of the transcript:

This means that this method is not good for getting isoform differences, but can
get good count data.


-----
10X is the predominant platform for single cell RNA analysis.

The technology uses barcoded beads in an emulsion.  Single cells bind to the
beads along with enzymes and reagents to prepare RNA.  

Each bead has thousands of oligos with a unique molecular identifier and a
barcode, so, you get:

    - [1] unique barcode for each cell
    - [2] unique molecular identifier for each RNA 
          + probably group transcripts
    - [3] also includes p5 and p7 illumina primers 


-----
PCR duplicates -> so, if 2 molecules have the same UMI for a read, we call that
a PCR duplicate.  The idea is that EVERY transcript needs to have a UNIQUE
identifier.  If there is ever more than UMI read, that must be a PCR
duplicate.  You'll see multiple copies of transcripts, but these will have
different UMIs and so the quantification will come from alignment.


-----
in slide 8, the thin gray line is your transcript. 

b/c of how long the adapters and barcodes are, you lose a bunch of information
from the paired-end sequencing.  BUT, you need the UMI and barcode to accurately
assign transcripts/cells, etc...


-----
result of alignment, umi/cell information retrieval is a count table (cells x
genes). 


-----
# best practices for single cell
https://github.com/theislab/single-cell-tutorial

two key types of analysis (see slides 13..14):

    - [1] visualization

    - [2] downstream analysis
          + trajectory inference
          + differential expression
          + gene dynamics
            - estimate direction cells are going in cell states/differentiation


-----
------------------
/Machine Learning/
------------------
# goal: learn from the data

reduce the data to be able to look at the data in a reduced dimension space

cluster: identify groups within the matrix that are similar to ea. other. 


-----
PCA - find a projection that preserves variability:

    - [1] find projection w/ highest variance
    - [2] find projection orthogonal to this
    - [3] repeat

/think about this in terms of looking at the data ala side 20 (left)/ 



-------
/T-SNE/
-------
# goal: take a hugely multidimensional space, reduce, and preserve neighbors in
# doing so.  

this is all T-SNE is.  

T-SNE is very much an art, apparently; result depends highly on tuning
parameter, which we set.  

"want to place points in the lower dimensional space such that the chance of a choosing b as its neighbor is preserved."


-----
------
/UMAP/
------
alternative to t-sne.  

distances actually mean something.  



*** Clustering

    - [] k-means clustering :: find the center of each cluster in a data set

    - [] hierarchical clustering :: find a way to define distance betw. points
                                    + define linkage
                                    + iteratively join points based on distance

    - [] graph clustering :: make a graph and form clusters by operating on the
                             graph (louvain - popular method?)

james recommends doing pca down to ~30 dimensions, THEN run the other clustering
methods to reduce further. 


-----
specific ss-seq recommendations see slide 37

* Tuesday, December 9 2019
** DONE Lecture: Chromatin States 2: Overlapping Datasets
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_ChIP-seq_histone_modifications_002.pdf
[[BED_tools][BED tools for intersect analysis]]

# key point: histone modifications co-localize with ea. other

look for co-localization of histone marks across chip-seq markers w/ heatmaps
(slide 8)


-----
some common questions:

    - [1] are enhancers cell-type specific?

    - [2] are enhancers poised or active?

    - [3] are enhancer TF motifs related to cell types?

    - [4] do enhancers overlap disease-associated SNPs? 
          + is overlap at a tf motif? 


-----
*BED_tools can be used to get intersects of overlaps*
/syntax for this actually looks pretty straightforward/
(slide 13)


-----
motif finding is often about finding nucleosome-free regions (slide 15)

see slide 16 for identifying a nucleosome-free region (it's the point where the
read density drops off big time for the chip seq).  


-----
next goal was to identify variants in enhancers/regulatory regions.  seems this
can create profound (morphological) phenotypes per slide 19. 

e.g., loss of enhancer in stickleback leads to loss of ventral fin


-----
in their version of the mpra, they do barcoding after DNA/RNA isolation.
collect DNA/RNA and use read counts (since these provide better quantitation) to
get enhancer activity. (slide 24)


-----
"*overlapping histone modifications best defines the state of the regulatory element*"



*** Looking for Silencer Elements
two forms of repression:

    - [1] passive :: protein sits on the DNA and blocks TF binding

    - [2] active :: repressor bound to silencer element does somthing to block
                    transcription 

simple subtractive approach. 


-----
some silencers are annotated as enhancers in other cell types.  this seems to
depend on differential binding of TFs at the same site in different cell types
(slide 54). 

** DONE Lecture: Probing Higher Dimension Chromatin Structure
file+emacs:/home/mahlon/reader/cshl_computational_genomics/CSHL_2019_Chromosome_Conformation_001.pdf

slide 2 of this is /beautiful/

-----
enhancers/silencers can be millions of bp away from their target gene/promoter.
how can this be?


-----
yellow locus in drosophila has multiple enhancers that have tissue specificity.
so the different ovals makr enhancers for a given tissue.  lose the tissue
specific activity of an enhancer when the retrotransposon inserts upstream of
the given enhancer.


-----
ctcf is a common insulator protein b/c it gives good chip reuslts.


-----
slide 13 - spacing the enhancer w/ 2 copies of supressor of hairy wing
("su(Hw)"), leads to restored enhancer activity.  suggests that
enhancer/promoter spacing matters.  see slide 17 for how this works.


-----
how do we link candidate regulatory elements to target genes (slide 23)

one approach is an eQTL approach where we link polymorphism in candidate
regulatory element (crm) to expression.  (gtex)


-----
many methods for assessing chromatin conformation (see slide 35)



*** Hi-C
restriction enzyme digest of cross-linked DNA.  involves biotin labeling of
junctions after ligation.  the biotin labeling helps w/ purifying the fragment
of interest.

want fragments that are ~250 bp for illumina sequencing

# key point
james asked how long fragments will be if we use restriction enzyme cutting of
random DNA.  so, the question is how many bases long the recognition sequence
is.  if they're 4 bp long, then the p() of that sequence in random DNA is:

1/(0.25^4) = 256 bp fragments, b/c:

equal p of any nucleotide, need 4 specific ones in a row to get the sequence

note how this changes for a 6 bp recognition sequence:

1/(0.25^6) = 4097. 


-----
"each end pair represents an interaction" (slides 43-44)


-----
use a correlation analysis to determine interaction profiles.  these results
(slide 46) suggest a "neighborness" to the chromosome wherein there are two
basic groups of interactions, so you're either highly correlated w/ group 1 or
highly anti-correlated w/ group 1 (red/blue of heat map).


-----
all of the structure in slide 48 has to get set and reset ea. time you go
through cell cycle.  


-----
modeling sources of error in hi-c and 5c:

expected interaction is a distance-dependent function plus bin-specific
correction factor.  bin factors are used to subtract or scale the model.  

the end result of the correction is removal of the vertical 'striping' of the
hi-c map.  the stripes are regions that don't sequence well for various reasons.


-----
single-cell hi-c

chromosome territories are essentially random in individual cells.  thus, the
interest in single-cell hi-c.  

"can't reconstruct territories from bulk data b/c you're averaging over many populations."

slide 80 shows the sort of A/B compartmentalization that's discussed at the
earlier part of the talk. 


-----
# key point: INTER-chromosomal contacts are RANDOM
#            INTRA-chromosomal contacts are highly NON-RANDOM



*** Mechanisms Underlying Chromosome Conformation
# key point: idea is that loops ar extruded from chromatin and bound by some
# protein complex that leads to them being stuck out from the rest of the
# chromosome (slide 91)

CTCF and cohesin are thought to be key players in this. 

see slide 92 for examples of different types of loops


-----
removing ctcf or cohesin leads to loss of local chromatin structure. 


-----
key ideas in the field so far:

    - [1] organization into A/B compartment 
          + correlated/anti-correlated

    - [2] organization into TADs

    - [3] more dynamic sub-TADs


*** Dynamics of B Compartment Organization
uses DamID approach, which tells you where proteins are interacting w/ DNA.  use
this w/ lamin a as POI (see slide 100) to understand more about where in the
nucleus the DNA lies.  


-----
cool method w/ chromosome paints where the lamin-associated domains (LADs) are by the
lamina and the non-LADs are further away.  The neat thing here is that they
paint all the lads the same color and the non-lads the same color.  
(slide 112-115). 

the live cell data suggests that the LAD/B chromatin is regulated immediately
after cell cycle exit.  you see this from aggregates that form in the nucleus
before they get to the lamin. 


-----
*key point: LADS are the chromatin B compartment*


-----
slide 131 has lots of hi-c tools

* Computational Fine-Mapping
** Links
[[https://www.ncbi.nlm.nih.gov/pubmed/31358562][arabidopsis paper]]
[[https://www.ncbi.nlm.nih.gov/pubmed/25633252/][cvd paper]]

https://galaxyproject.github.io/training-material/
# training

https://galaxyproject.org/toolshed/
# galaxy toolshed

file+emacs:/home/mahlon/reader/cshl_computational_genomics/project/final_QTL_bed.bed
# final bed file

goals:

    - [1] use vcf file to annotate variants
    - [2] get list of genes
    - [3] plot variant locations
    - [4] get expression data
    - [5] predict pathogenicity
    - [6] functional information
          + maybe also deletion phenotype

*** Steps [66%]

    - [X] create bed file w/ the regions of interest in it.
          + bed file format is:
            - 1. chromosome ("Chrom")
            - 2. Start
            - 3. End
	    - 4. gene/region (mine is QTL but usually would be gene)
          + making the bed
            - use the QTL table from you R output (multipool peaks)
	    - name the header row
	    - named my 'genes' QTL-1..11

:final_ucsc_bed:
#chrom chromStart chromEnd name
chrI	0	58200	QTL-1   
chrIV	349700	497400	QTL-2
chrIV	995600	1174300	QTL-3
chrV	116200	258600	QTL-4
chrVII	847200	877100	QTL-5
chrVIII	138000	220300	QTL-6
chrXII	174900	254900	QTL-7
chrXII	863000	954800	QTL-8
chrXIII	18400	106100	QTL-9
chrXIV	467600	488500	QTL-10
:END:

    - [X] filter SNPs that aren't from RM

    - [X] use a tool to subset the vcf (SNPs) using the QTL regions (BED)
          + snpsift interals on the bed file made from multipool peaks
          + filtered intervals

    - [X] plot SNPs against genome to visualize gene/variant positions
          + ucsc genome browser
            - custom tracks 

    - [ ] get interesting features and information from QTL regions
          * intersect SNPs in regions w/ features 
          + % SNPs in coding vs. non-coding
            - gtf/gff/bed intersect w/ SNPs in regions 
          + oreganno feature analysis
          + eran segal reg. mod. table (TF binding sites)
          + snpeff
            - make sure to have no upstream/downstream intervals

    - [ ] drop random QTLs over the genome
          + shufflebed
          + do same analysis as above

    - [ ] plot of features in oreganno (yeast vs. all)

    - [ ] sift analysis for pathogenicity prediction

    - [ ] make a bargraph of oreganno annotations for yeast/all
          + quantitative data

    - [ ] some type of QTG prediction
          + expression
	  + proximity to peak 
	  + conservation 
	  + n. SNPs in gene/promoter
          + 


my output:

    - [1] QTL regions and SNPs in regions overlaid on UCSC
          + create plots from this
          + also include QTL plots from poster

    - [2] SNPs in QTLs annotated and statistically compared to random regions
          + snpeff
          + oreganno


** Methods

    - [1] take list of genes in region, examine
          + effect of polymorphism
          + sequence conservation
          + regulatory information
          + expression profile
            - i.e., not expressed, not QTG
          + gene ontology
          + KEGG
          + *N variants*

    - [2]



** Approach
Basically, you'd need a set of QTL regions and known causal genes.
With these, you would be able to develop an algorithm that ranks genes
in a region and then can benchmark accordingly.  Will need a large
number of QTGs though.

* Code
-----
#+NAME: r_markdown_to_org
#+BEGIN_SRC R

## install.packages("knitr")
library(knitr)

## install.packages("markdown")
library(markdown)

## create a markdown file for pandoc conversion
knit("~/reader/cshl_computational_genomics/BIMS_R_Tutorial_2017.Rmd",
     "~/reader/cshl_computational_genomics/BIMS_R_Tutorial_2017.md")


## -----
## jeff leek's stuff
setwd("~/reader/cshl_computational_genomics/cshlcg-labs/")
knit("about_us_2019.Rmd", "about_us_2019.md")
knit("cshl-lab1-2018.Rmd", "cshl-lab1-2018.md")
knit("cshl-lab1-2019.Rmd", "cshl-lab1-2019.md")
knit("cshl-lab2-2018.Rmd", "cshl-lab2-2018.md")
knit("cshl-lab2-2019.Rmd", "cshl-lab2-2019.md")

as.numeric(as.roman("III"))
a <- c("chrI", "chrII", "chrIII", "chrIV")
a_split <- strsplit(x = a, split = "chr")

as.vector(lapply(1:3, function(x){as.numeric(as.roman(a_split[[x]][2]))}))

#+END_SRC


-----
#+NAME: pandoc_md_to_org
#+BEGIN_SRC bash

cd ~/reader/cshl_computational_genomics
pandoc -o ./BIMS_R_Tutorial_2017.org ./BIMS_R_Tutorial_2017.md

cd ~/reader/cshl_computational_genomics/cshlcg-labs
pandoc -o ./cshl-lab1-2018.org ./cshl-lab1-2018.md
pandoc -o ./cshl-lab1-2019.org ./cshl-lab1-2019.md
pandoc -o ./cshl-lab2-2018.org ./cshl-lab2-2018.md
pandoc -o ./cshl-lab2-2019.org ./cshl-lab2-2019.md


#+END_SRC


-----
#+NAME: devtools_session_info
#+BEGIN_SRC R
## -----
## the session info command
## this gives a lot of useful info, like loaded packages,
## computer info, r versoin, etc...
library("devtools")
session_info()


## -----
## ensure that we load the package w/o a call to "library"
devtools::session_info()

#+END_SRC


#+NAME: convert bed to roman numerals
#+BEGIN_SRC R
## -----
## setup
setwd("~/reader/cshl_computational_genomics/project")


## -----
## so, the goal here is to convert a bed file into a format
## that galaxy/snpsift likes
## start by getting the information into a vector and convert
## then replace the first column with the correctly formatted one
bed <- read.table("test_oreganno_bed.bed", header = T)
roman.convert <- function(x){paste("chr", as.character(
                                              as.roman(
                                                  as.numeric(
                                                      sub("chr", "", x)))), sep = "")}

r.column <- roman.convert(bed$Chr)
bed$Chr <- r.column


## -----
## now write the output
write.table(x = bed, file = "formatted_oreganno_bed.bed", append = F, sep = "\t", quote = F, row.names = F)

#+END_SRC

* Books

** Books on Computational Biology and Molecular Evolution
(by publication date)

Introduction to Protein-DNA Interactions: Structure, Thermodynamics, and
Bioinformatics (2013) G. Stormo, CSHL Press ISBN: 1936113503

Practical Computing for Biologists (2010) S. Haddock and C. Dunn Sinauer ISBN:
0878933913

Bioinformatics and Functional Genomics (2009) Johnathan Pevsner, Wiley ISBN:
0470085851

Bioinformatics: Sequence and Genome Analysis, (2004) David W. Mount Cold Spring
Harbor Press ISBN:0879697121 ISBN: 0879696087.

Bioinformatics : A Practical Guide To The Analysis Of Genes And Proteins, 3rd
Ed. (2004) Baxevanis, Andreas (Ed)/ Ouellette, B. F. Francis (Ed),
Wiley-Interscience ISBN: 0471478784

Bioinformatics for Dummies (2003) Claverie, Jean-Michel and Notredame, Cedric
published 1/2003 ISBN: 0764516965

Statistical Methods in Bioinformatics by Warren J. Ewens, Gregory R. Grant
published 4/2001; ISBN: 0387952292

Computational Methods in Molecular Biology, by Salzberg, Steven (Ed), Searls,
D. B., and Kasif, S. published 02/1999, ISBN: 0444502041

Biological Sequence Analysis : Probabilistic Models Of Proteins And Nucleic
Acids, by Durbin, Richard (Edt), Eddy, R., Krogh, A., Mitchison, G., Durbin, R.,
published: 05/1998, ISBN: 0521629713.

Bioinformatics; The Machine Learning Approach, by Baldi, Pierre (Other:Brunak,
Soren Joint Author:Brunak, Soren), published: 01/1998, ISBN: 026202442X.

Algorithms On Strings, Trees, And Sequences : Computer Science And Computational
Biology, by Gusfield, Dan, published: 06/1997, ISBN: 0521585198 .

Computer Methods For Macromolecular Sequence Analysis (Methods In Enzymology,
Vol 266), by Doolittle, Russell F. (Edt), published: 05/1996, ISBN: 0121821676.

Introduction To Computational Biology; Maps, Sequences, And Genomes, by
Waterman, Michael S., published: 03/1995, ISBN: 0412993910.

** Molecular Evolution

Fundamentals Of Molecular Evolution, by Li, Wen-Hsiung/ Graur, Dan, published:
10/1999, ISBN: 0878932666

Molecular Systematics, by Hillis, David M. (Edt)/ Moritz, Craig (Edt)/ Mable,
Barbara K. (Edt), published: 10/1997, ISBN: 0878932828.

* DONE Packing List [100%]
    - [X] phone
    - [X] cards
    - [X] poster
    - [X] phone charger
    - [X] shirts
    - [X] underwear
    - [X] pants
    - [X] socks
    - [X] running shoes
    - [X] running gear
    - [X] watch
    - [X] book
    - [-] keyboard
    - [X] screen protector
    - [X] laptop charger
    - [X] toiletries


* Journal
I just completed the Cold Spring Harbor Course "Computational Genomics".  It's
hard to believe how fortunate I am to have had all the experiences I've had as
part of my training as a scientist.  In some ways, I feel I am not worthy of the
honor of the experiences.  This course was part of a fellowship training plan
and I even received funds to sponsor my attendance.  I am grateful for this and
the many blessings in my life.  It is a good reminder of how fortunate I am that
i was surrounded at the course by individuals from countries as far away as
Argentina, Austrailia, and Kenya.  

The course began with an early Tuesday morning flight.  The flight itself left
at 7 a.m. or so and we left, per usual, later than was probably wise.  My last
few experiences with security at the MSP airport.  This is almost certainly
because I fly out late at night to the CIU airport when few flights are left to
depart other than mine.  The opposite is, of course, true early in the morning.
as a result, I had to stand in line for 45 minutes or so.  I actually ran to my
gate, lugging my roller bag, laptop bag, and poster all the way.  I arrived and
boarded as one of the last passengers.  A mistake here was letting the gate
agent check my bag.  I was frustrated with myself on entering the plane and
realizing that there were, in fact, several places that would have accommodated
my bag.  This frustration was exacerbated when I had to wait approximately 20
minutes for my bag to arrive on the baggage check conveyor belt.   I've resolved
to remove the tag from the bag and just roll the dice on there being space in
the overhead.  This seems reasonable inasmuch as the flight attendants have to
make the call early to start checking bags, as many passengers would get on with
roller bags and no overhead space otherwise.  

I arrived at CSHL on Tuesday around 1:00 p.m.  I was thankful that I had not
missed lunch and set about to the usual copious eating that accompanies the CSHL
experience.  There was a meeting on plant genetics happening at the same time as
our course and the food was plentiful as a result.  For the course, we were
housed in cabins, as I was for the ubiquitin meeting in May.  The cabin really
adds to the experience and the parallels between the CSHL experience and the
summer camp experience were very evident to me after this past week, moreso than
in my previous two visits to CSHL.  I attempted to work on my grant for the
remainder of the afternoon before going to dinner.  This was not a great
success.  My work was periodically interrupted by the arrival of a new course
participant.  I was moderately dismayed to learn that we would have roommates in
our bedrooms, but this didn't prove to be a major inconvenience.  One my cabin
mates, Dan, is from Ottawa and had one of the thickest Canadian accents I've
ever heard.  I was later asking him if he spoke french to which he replied
"praire french", which I found amusing.  

The remainder of the day didn't consist of much activity outside of dinner and a
reception in the bar.  The prevalent bar culture of CSHL continues to be a
source of confusion for me.  I see no obvious appeal to waking up hung over for
a day of talks and lectures.  Still, the drinks were free and we settled down as
a group in the bar.  Some of the cultural artifacts of CSHL were more obvious to
me on this trip.  In particular, I noticed a guitar that Francis Collins
evidently played during the announcement of the completion of human genome
project that is encased in the bar.  The conversation in our group was stilted,
as might be expected for a group of people meeting for the first time.  One of
the more amusing aspects of these sorts of things is how we all cluster to those
most like ourselves.  I found myself gravitating to Dan, figuring him to be a
like-minded mid-westerner.  Likewise, the asian contingent (which was
considerable) also grouped together.  After a single drink, we all departed for
in order to get enough sleep for the next 7 days.  

The next day, the course began in proper.  I was immediately annoyed to hear the
instructors pushing the
"we're going to be working 15 hour days and you're going to be exhausted by week's end"
schtick.  In the end, this statement was entirely true, but the celebration of
it always is irksome to me.  The course is primarily run by William 'Bill'
Pearson, whose main claim to fame seems to be inventing the FASTA file format.
while this is, without question, a major claim to fame, he was remarkably modest
about it, remarking with deprecating humor, that his approach was to add a
single line to an existing file format.  He talked about how in the days prior
to FASTA, people assembled their own protein sequence databases and to use one,
you had to request access AND the database maintainer to the resultant
publication.  Bill began the course with lectures on protein sequence similarity
searching.  One surprise was his emphatic emphasis on the limited utility of DNA
databases for detecting homology.  

A key concept that began to make much more sense through these lectures is the
idea of the value of homology.  I was vaguely aware of the idea of homology,
which refers to sequences originating from a common ancestor.  Bill's lecture
helped clarify how this concept has come to occupy a central role in molecular
biology/genetic theory.  The basic idea is that the amount of protein sequence
similarity observed across domains of life vastly exceeds that which would be
expected by chance.  Thus, there are two possibilities: nature has independently
evolved the same proteins in many domains of life, or they arise from a common
ancestor.  Because the latter is the more parsimonious explanation, it has
become the accepted view in the field (though I suspect independent origin of
function is seen as well).  The origin from a common ancestor means that any two
homologues are likely to share function (or, minimally, functional domains).
this means that homology can be used to infer function in proteins/genes where
this is not known.  And, indeed, it seems that a considerable number (more than
half, perhaps) of known genes have no function assigned to their protein product
(or the function is inferred on the basis of homology).  

The concepts in Bill's lectures were difficult to grasp and I found myself
wrestling with them throughough the course.  The laboratories he developed were
filled with a mixture of conclusions that seemed based, in parts, on pure
statistical/quantitative conclusions and those that were based on 'common sense'
(i.e., what is the meaning of the highest scoring non-homolog).  

the day proceeded with more lectures on sequence similarity and homology
searching.  these really were difficult concepts to grasp and i was feeling
discouraged and with a bit of imposter syndrome by day's 




* CSHL Questionnaire
** DONE 1. Introductions First:
a. With which institution are you affiliated?
The University of Minnesota

b. Are you part of a specific lab there?
I'm a member Dr. Frank Albert's laboratory in the Department of Genetics, Cell
Biology, and Development.

c. What's your position?
I'm a postdoctoral researcher. 

d. If you’d like for me to tag your public Twitter and/or Instagram account,
what is your handle? (And feel free to send me the handle for your lab and/or
institution as well.)
n/a

** DONE 2. Aid
2. Did you receive any sort of financial aid from CSHL to attend this course? If
   yes, would it be alright to mention it in your feature? (I can find out the
   name of your grantor so don’t worry if you can’t remember its name.)
n/a

** TODO 3. Work
3. Let’s talk about your work:

a. What are your research interests? What are you working on? (When answering
this, please keep a general science audience in mind and to a max of 2
sentences.)

I study how genetic variation affects cellular physiology.  My current work
focuses on the role of individual genetic differences influence protein
degradation, an essential biological process implicated in a variety of
diseases, including neurodegenerative diseases, cancers, and immune disorders.

b. How did you know you wanted to study this/make it the focus of your research?
My doctoral research focused on neurodegenerative diseases, including
amyotrophic lateral sclerosis (ALS) and dementia.  I was surprised to learn 

c. How did your scientific journey begin? (In case this seems similar to 3b,
this question is asking about how your career in science started: what or who
inspired you.)

I grew up on a small family farm in t



** TODO 4-6 CSHL
4. For your current course, was there something specific about it that drew you
   to apply? Or was there a skill set/lab technique you wanted to acquire that
   this course covers?

I

a. From what you’ve so far learned from the course, what/how will you apply it
to your work, with your lab mates, and/or at your home institution?

5. What is your key takeaway from the Course?



6. Let’s talk about your Meetings & Courses history: 

a. How many CSHL courses have you attended?
This is my first.  I wish I had taken one sooner! 

b. Have you attended a CSHL meeting? If yes, how many/which one(s)?
I attended the 2018 'Biology of Genomes' meeting and the 2019 'Ubiquitins,
Autophagy, and Disease' meeting.  

c. Will you be attending any other near future CSHL courses and/or meetings?
Yes, I'm very much looking forward to future 'Biology of Genomes', 'Ubiquitins,
Autophagy, and Disease', and other meetings at CSHL.

7. If someone curious in attending this course asked you for feedback or advice
   on it, what would you tell him/her?

First, if you're thinking about taking the course, do it.  It's both an
excellent training in the foundations of the 

training in foundational and survey of cutting edge.  a foundation for further
study in computational biology

8. What do you like most about your time at CSHL? (Please answer this question
   with something not science related.)

Life at CSHL is like attending summer camp and my favorite aspect is the sense
of community and camaderie.  This starts with living together in cabins and
builds through the unique experiences you have while at CSHL.  I have fond
memories of long, early-morning runs led by Randy XXX at the 'Ubiquitins,
Autophagy, and Disease' meeting and, now, of James Taylor introducing our group
to Little xxxx 'Long Island cold cheese pizza' while we worked late into the
night on our course projects.  That you can have these experiences while
interacting with leaders in your field is truly remarkable and unlike any other
meeting or course I've attended.
